"""
AI íŠ¸ë Œë“œ ì˜ˆì¸¡ ë³´ê³ ì„œ ìƒì„± AI ì—ì´ì „íŠ¸
LangGraph êµ¬í˜„ with Tavily Search
"""

from typing import TypedDict, Annotated, List, Dict, Optional, Any
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
import operator
import os
import re
from datetime import datetime
from dotenv import load_dotenv

# PDF ìƒì„± ë¼ì´ë¸ŒëŸ¬ë¦¬
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

from pathlib import Path
# ==================== í•œê¸€ í°íŠ¸ ì„¤ì • ====================
FONT_DIRS = [
    Path(os.environ.get("LOCALAPPDATA", "")) / "Microsoft" / "Windows" / "Fonts",
    Path(r"C:\Windows\Fonts"),
]

def find_font(filename):
    for base in FONT_DIRS:
        candidate = base / filename
        if candidate.exists():
            return str(candidate)
    raise FileNotFoundError(filename)


# ==================== PDF ì„¤ì • ====================
# í•œê¸€ í°íŠ¸ ë“±ë¡
try:
    pdfmetrics.registerFont(TTFont("NanumGothic", find_font("NanumGothic-Regular.ttf")))
    pdfmetrics.registerFont(TTFont("NanumGothicBold", find_font("NanumGothic-Bold.ttf")))
    KOREAN_FONT = 'NanumGothic'
    KOREAN_FONT_BOLD = 'NanumGothicBold'
except:
    KOREAN_FONT = 'Helvetica'
    KOREAN_FONT_BOLD = 'Helvetica-Bold'
    print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

load_dotenv(override=True)

# ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ (ë” ë‹¤ì–‘í•œ ìƒ‰ìƒ)
PRIMARY_COLOR = colors.HexColor('#1E3A8A')      # ì§„í•œ íŒŒë‘
SECONDARY_COLOR = colors.HexColor('#3B82F6')    # ë°ì€ íŒŒë‘
ACCENT_COLOR = colors.HexColor('#06B6D4')       # ì²­ë¡ìƒ‰
SUCCESS_COLOR = colors.HexColor('#10B981')      # ì´ˆë¡ìƒ‰
WARNING_COLOR = colors.HexColor('#F59E0B')      # ì£¼í™©ìƒ‰
DANGER_COLOR = colors.HexColor('#EF4444')       # ë¹¨ê°•ìƒ‰
LIGHT_BG = colors.HexColor('#F0F9FF')           # ì—°í•œ íŒŒë‘ ë°°ê²½
GRAY_BG = colors.HexColor('#F3F4F6')            # íšŒìƒ‰ ë°°ê²½
BORDER_COLOR = colors.HexColor('#E5E7EB')       # í…Œë‘ë¦¬ ìƒ‰


class NumberedCanvas(canvas.Canvas):
    """í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ìº”ë²„ìŠ¤"""
    def __init__(self, *args, **kwargs):
        canvas.Canvas.__init__(self, *args, **kwargs)
        self._saved_page_states = []
        
    # def showPage(self):
    #     self._saved_page_states.append(dict(self.__dict__))
    #     self._startPage()
        
    def showPage(self):
        """í˜ì´ì§€ë¥¼ ì €ì¥í•˜ê¸° ì „ì— í˜ì´ì§€ ë²ˆí˜¸ ê·¸ë¦¬ê¸°"""
        page_num = self._pageNumber
        
        # ì²« í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ í˜ì´ì§€ ë²ˆí˜¸ ê·¸ë¦¬ê¸°
        if page_num > 1:
            self.saveState()  # í˜„ì¬ ìƒíƒœ ì €ì¥
            self.setFont(KOREAN_FONT, 9)
            self.setFillColor(colors.grey)
            # ìš°ì¸¡ í•˜ë‹¨
            self.drawRightString(A4[0] - 50, 25, f"Page {page_num - 1}")
            # ì¢Œì¸¡ í•˜ë‹¨
            self.drawString(50, 25, "Physical AI Trend Report")
            self.restoreState()  # ìƒíƒœ ë³µì›
        
        # ì‹¤ì œ í˜ì´ì§€ ì €ì¥
        canvas.Canvas.showPage(self)
    # def save(self):
    #     num_pages = len(self._saved_page_states)
    #     for state in self._saved_page_states:
    #         self.__dict__.update(state)
    #         self.draw_page_number(num_pages)
    #         canvas.Canvas.showPage(self)
    #     canvas.Canvas.save(self)
        
    # def draw_page_number(self, page_count):
    #     """í˜ì´ì§€ ë²ˆí˜¸ ê·¸ë¦¬ê¸°"""
    #     page_num = self._pageNumber
    #     if page_num > 1:
    #         self.setFont(KOREAN_FONT, 9)
    #         self.setFillColor(colors.grey)
    #         self.drawRightString(A4[0] - 50, 25, f"Page {page_num - 1} of {page_count - 1}")
    #         self.drawString(50, 25, "Physical AI Trend Report")


def clean_markdown_symbols(text):
    """ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ë¥¼ ì œê±°í•˜ê³  ì •ë¦¬"""
    # ë³¼ë“œ ì²˜ë¦¬ (**text** -> <b>text</b>)
    text = re.sub(r'\*\*\*\*(.*?)\*\*\*\*', r'<b>\1</b>', text)
    text = re.sub(r'\*\*\*(.*?)\*\*\*', r'<b><i>\1</i></b>', text)
    text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', text)
    
    # ì´íƒ¤ë¦­ ì²˜ë¦¬ (*text* -> <i>text</i>)
    text = re.sub(r'\*(.*?)\*', r'<i>\1</i>', text)
    
    # ì¸ë¼ì¸ ì½”ë“œ ì²˜ë¦¬ (`code` -> <font name="Courier">code</font>)
    text = re.sub(r'`(.*?)`', r'<font name="Courier" color="#E11D48">\1</font>', text)
    
    # ë§í¬ ì²˜ë¦¬ [text](url) -> text
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)
    
    return text


def parse_markdown_list_item(line, level=0):
    """ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ íŒŒì‹± (ë“¤ì—¬ì“°ê¸° ë ˆë²¨ í¬í•¨)"""
    # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸: -, *, â€¢
    bullet_match = re.match(r'^(\s*)([-*â€¢])\s+(.+)$', line)
    if bullet_match:
        indent = len(bullet_match.group(1))
        content = bullet_match.group(3)
        level = indent // 2  # ë“¤ì—¬ì“°ê¸° ë ˆë²¨ ê³„ì‚°
        return 'bullet', content, level
    
    # ìˆ«ì ë¦¬ìŠ¤íŠ¸: 1., 2., etc.
    number_match = re.match(r'^(\s*)(\d+)\.\s+(.+)$', line)
    if number_match:
        indent = len(number_match.group(1))
        number = number_match.group(2)
        content = number_match.group(3)
        level = indent // 2
        return 'number', content, level, number
    
    return None, None, 0


def markdown_to_pdf(markdown_text: str, output_filename: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ë³€í™˜"""
    
    doc = SimpleDocTemplate(
        output_filename,
        pagesize=A4,
        rightMargin=50,
        leftMargin=50,
        topMargin=50,
        bottomMargin=50
    )
    
    # ìŠ¤íƒ€ì¼ ì •ì˜
    styles = getSampleStyleSheet()
    
    # ë©”ì¸ íƒ€ì´í‹€ ìŠ¤íƒ€ì¼
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Title'],
        fontSize=32,
        textColor=PRIMARY_COLOR,
        alignment=TA_CENTER,
        spaceAfter=40,
        spaceBefore=20,
        fontName=KOREAN_FONT_BOLD,
        leading=38
    )
    
    # H1 ìŠ¤íƒ€ì¼ (ë°•ìŠ¤ í˜•íƒœ)
    h1_style = ParagraphStyle(
        'CustomH1',
        parent=styles['Heading1'],
        fontSize=22,
        textColor=colors.white,
        spaceAfter=15,
        spaceBefore=25,
        fontName=KOREAN_FONT_BOLD,
        borderWidth=0,
        borderPadding=12,
        backColor=PRIMARY_COLOR,
        leftIndent=10,
        rightIndent=10
    )
    
    # H2 ìŠ¤íƒ€ì¼ (ì™¼ìª½ í…Œë‘ë¦¬)
    h2_style = ParagraphStyle(
        'CustomH2',
        parent=styles['Heading2'],
        fontSize=18,
        textColor=PRIMARY_COLOR,
        spaceAfter=12,
        spaceBefore=18,
        fontName=KOREAN_FONT_BOLD,
        leftIndent=15,
        borderWidth=0,
        borderPadding=8,
        borderColor=ACCENT_COLOR,
        backColor=LIGHT_BG
    )
    
    # H3 ìŠ¤íƒ€ì¼
    h3_style = ParagraphStyle(
        'CustomH3',
        parent=styles['Heading3'],
        fontSize=15,
        textColor=SECONDARY_COLOR,
        spaceAfter=10,
        spaceBefore=14,
        fontName=KOREAN_FONT_BOLD,
        leftIndent=10
    )
    
    # H4 ìŠ¤íƒ€ì¼
    h4_style = ParagraphStyle(
        'CustomH4',
        parent=styles['Heading3'],
        fontSize=13,
        textColor=SECONDARY_COLOR,
        spaceAfter=8,
        spaceBefore=12,
        fontName=KOREAN_FONT_BOLD,
        leftIndent=5
    )
    
    # ë³¸ë¬¸ ìŠ¤íƒ€ì¼
    body_style = ParagraphStyle(
        'CustomBody',
        parent=styles['Normal'],
        fontSize=11,
        alignment=TA_JUSTIFY,
        spaceAfter=10,
        leading=18,
        fontName=KOREAN_FONT,
        textColor=colors.HexColor('#374151')
    )
    
    # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
    bullet_style = ParagraphStyle(
        'BulletList',
        parent=body_style,
        leftIndent=20,
        bulletIndent=10,
        spaceAfter=6,
        fontSize=11
    )
    
    # ì¸ìš©êµ¬ ìŠ¤íƒ€ì¼
    quote_style = ParagraphStyle(
        'Quote',
        parent=body_style,
        leftIndent=30,
        rightIndent=30,
        fontSize=10,
        textColor=colors.HexColor('#6B7280'),
        backColor=GRAY_BG,
        borderWidth=0,
        borderPadding=10,
        borderColor=ACCENT_COLOR,
        spaceAfter=12,
        spaceBefore=12
    )
    
    story = []
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
    lines = markdown_text.split('\n')
    i = 0
    in_quote = False
    quote_lines = []
    
    while i < len(lines):
        line = lines[i].rstrip()
        
        # ë¹ˆ ì¤„
        if not line:
            if in_quote and quote_lines:
                # ì¸ìš©êµ¬ ì¢…ë£Œ
                quote_text = '<br/>'.join(quote_lines)
                story.append(Paragraph(quote_text, quote_style))
                quote_lines = []
                in_quote = False
            i += 1
            continue
        
        # ì¸ìš©êµ¬ ì²˜ë¦¬
        if line.startswith('> '):
            in_quote = True
            quote_content = line[2:].strip()
            quote_content = clean_markdown_symbols(quote_content)
            quote_lines.append(quote_content)
            i += 1
            continue
        elif in_quote:
            # ì¸ìš©êµ¬ ì¢…ë£Œ
            quote_text = '<br/>'.join(quote_lines)
            story.append(Paragraph(quote_text, quote_style))
            quote_lines = []
            in_quote = False
        
        # ì œëª© ì²˜ë¦¬
        if line.startswith('# '):
            title_text = line[2:].strip()
            title_text = clean_markdown_symbols(title_text)
            if i < 3:  # ë©”ì¸ íƒ€ì´í‹€
                story.append(Spacer(1, 0.3*inch))
                story.append(Paragraph(title_text, title_style))
                
                # ì œëª© ì•„ë˜ êµ¬ë¶„ì„ 
                line_table = Table([['']], colWidths=[doc.width])
                line_table.setStyle(TableStyle([
                    ('LINEABOVE', (0, 0), (-1, 0), 3, ACCENT_COLOR),
                    ('TOPPADDING', (0, 0), (-1, -1), 10),
                ]))
                story.append(line_table)
                story.append(Spacer(1, 0.2*inch))
            else:
                story.append(Paragraph(title_text, h1_style))
            i += 1
            
        elif line.startswith('## '):
            title_text = line[3:].strip()
            title_text = clean_markdown_symbols(title_text)
            story.append(Paragraph(f'â–¸ {title_text}', h2_style))
            i += 1
            
        elif line.startswith('### '):
            title_text = line[4:].strip()
            title_text = clean_markdown_symbols(title_text)
            story.append(Paragraph(f'â–ª {title_text}', h3_style))
            i += 1
            
        elif line.startswith('#### '):
            title_text = line[5:].strip()
            title_text = clean_markdown_symbols(title_text)
            story.append(Paragraph(f'â€¢ {title_text}', h4_style))
            i += 1
        
        # ìˆ˜í‰ì„ 
        elif line.startswith('---') or line.startswith('___'):
            hr_table = Table([['']], colWidths=[doc.width])
            hr_table.setStyle(TableStyle([
                ('LINEABOVE', (0, 0), (-1, 0), 1, BORDER_COLOR),
                ('TOPPADDING', (0, 0), (-1, -1), 5),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 5),
            ]))
            story.append(hr_table)
            i += 1
        
        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        else:
            list_type, content, level, *extra = parse_markdown_list_item(line) + (None,)
            
            if list_type in ['bullet', 'number']:
                content = clean_markdown_symbols(content)
                indent = 20 + (level * 20)
                
                if list_type == 'bullet':
                    bullet_char = 'â€¢' if level == 0 else ('â—¦' if level == 1 else 'â–ª')
                    list_text = f'{bullet_char} {content}'
                else:
                    list_text = f'{extra[0]}. {content}'
                
                list_para_style = ParagraphStyle(
                    f'ListLevel{level}',
                    parent=bullet_style,
                    leftIndent=indent,
                    bulletIndent=indent - 10
                )
                story.append(Paragraph(list_text, list_para_style))
                i += 1
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸
                cleaned_line = clean_markdown_symbols(line)
                story.append(Paragraph(cleaned_line, body_style))
                i += 1
    
    # ì¸ìš©êµ¬ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì²˜ë¦¬
    if in_quote and quote_lines:
        quote_text = '<br/>'.join(quote_lines)
        story.append(Paragraph(quote_text, quote_style))
    
    # ë°œí–‰ì¼ ì¶”ê°€
    story.append(Spacer(1, 0.5*inch))
    date_table = Table(
        [[Paragraph(f"<i>ë³´ê³ ì„œ ìƒì„±ì¼: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}</i>", 
                   ParagraphStyle('DateStyle', parent=body_style, fontSize=10, 
                                textColor=colors.grey, alignment=TA_CENTER))]],
        colWidths=[doc.width]
    )
    date_table.setStyle(TableStyle([
        ('LINEABOVE', (0, 0), (-1, 0), 1, BORDER_COLOR),
        ('TOPPADDING', (0, 0), (-1, -1), 10),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
    ]))
    story.append(date_table)
    
    # PDF ìƒì„±
    doc.build(story, canvasmaker=NumberedCanvas)
    
    return output_filename



# ==================== ë¦¬ì„œì¹˜ ê³„íš êµ¬ì¡° ====================
class ResearchArea(BaseModel):
    """LLMì´ ë°˜í™˜í•˜ëŠ” ê° ì—°êµ¬ ì˜ì—­ êµ¬ì¡°"""
    focus_question: str = Field(..., description="í•´ë‹¹ ì˜ì—­ì—ì„œ ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸")
    search_keywords: List[str] = Field(..., description="í•´ë‹¹ ì˜ì—­ì„ íƒìƒ‰í•  ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ (ìµœì†Œ 5ê°œ)")
    expected_insights: str = Field(..., description="ì¡°ì‚¬ë¥¼ í†µí•´ ë„ì¶œí•˜ê³ ì í•˜ëŠ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸")


class ResearchPlan(BaseModel):
    """LLMì´ ìƒì„±í•˜ëŠ” ì „ì²´ ë¦¬ì„œì¹˜ ê³„íš"""
    plan_overview: str = Field(..., description="ì „ì²´ ë¦¬ì„œì¹˜ ë°©í–¥ê³¼ ëª©í‘œ ìš”ì•½")
    market: ResearchArea
    tech: ResearchArea
    industry: ResearchArea
    company: ResearchArea
    challenge: ResearchArea


DEFAULT_RESEARCH_PLAN: Dict[str, List[str]] = {
    "market": [
        "Physical AI market size 2025-2030",
        "humanoid robotics market forecast",
        "AI robotics investment trends",
        "Physical AI market growth rate",
        "robotics market segmentation"
    ],
    "tech": [
        "Vision-Language-Action models 2025",
        "Physical AI foundation models",
        "robotics AI breakthrough 2025",
        "NVIDIA Isaac GR00T",
        "world foundation models robotics"
    ],
    "industry": [
        "Physical AI manufacturing use cases",
        "robotics logistics warehouse 2025",
        "AI robotics healthcare applications",
        "autonomous vehicles Physical AI",
        "retail robotics deployment"
    ],
    "company": [
        "Tesla Optimus humanoid robot",
        "Figure AI BMW partnership",
        "Agility Robotics Digit",
        "Boston Dynamics Atlas electric",
        "NVIDIA Cosmos robotics"
    ],
    "challenge": [
        "Physical AI adoption barriers",
        "robotics battery life challenges",
        "AI robot safety concerns",
        "workforce robotics impact",
        "Physical AI regulation 2025"
    ]
}

planning_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ AI ë¶„ì•¼ì˜ ì‹œë‹ˆì–´ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
í–¥í›„ 5ë…„ì„ ëŒ€ìƒìœ¼ë¡œ ì‹œì¥, ê¸°ìˆ , ì‚°ì—…, ê¸°ì—…, ë„ì „ê³¼ì œ ë‹¤ì„¯ ì˜ì—­ì— ëŒ€í•œ ë¦¬ì„œì¹˜ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
ì‘ë‹µì€ JSON í˜•ì‹ì´ë©°, ê° ì˜ì—­ì— ëŒ€í•´ focus_question, search_keywords, expected_insightsë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
search_keywordsëŠ” ì˜ì–´ë¡œ ì œê³µí•˜ë©° í•­ìƒ 5ê°œ ì´ìƒì˜ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë˜ëŠ” ë¬¸êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""),
    ("user", """ìš”ì²­: {query}

ë¶„ì„ ê¸°ê°„: í–¥í›„ 5ë…„

ê° ì˜ì—­ë³„ ì¡°ì‚¬ ëª©í‘œë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ê³ , ê²€ìƒ‰ í‚¤ì›Œë“œ 5ê°œì™€ ê¸°ëŒ€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œì‹œí•˜ì„¸ìš”.""")
])

planning_chain = planning_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0).with_structured_output(ResearchPlan)

QUALITY_CRITERIA = {
    "min_items": 3,
    "min_hit_ratio": 0.6,
    "min_avg_score": 0.25,
    "min_answer_ratio": 0.4,
    "min_content_ratio": 0.5,
    "min_answer_chars": 80,
    "min_content_chars": 120,
}

REVIEW_BASELINE_REPORT = """
AI ë³´ê³ ì„œ ìš”ì•½ (ë¯¸í¡ ì˜ˆì‹œ - ì ìˆ˜ 5.8/10)

ì‹œì¥ ì „ë§
AI ì‹œì¥ì€ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë§ì€ ê¸°ì—…ë“¤ì´ ì´ ë¶„ì•¼ì— íˆ¬ìí•˜ê³  ìˆìœ¼ë©°, í–¥í›„ ì „ë§ì´ ë°ìŠµë‹ˆë‹¤.

ê¸°ìˆ  íŠ¸ë Œë“œ
- AI ê¸°ìˆ 
- ë¡œë´‡ ê¸°ìˆ 
- ì„¼ì„œ ê¸°ìˆ 

ì‚°ì—… ì ìš©
ì œì¡°, ë¬¼ë¥˜, í—¬ìŠ¤ì¼€ì–´ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

ê¶Œê³ ì‚¬í•­
ê¸°ì—…ë“¤ì€ AI ê¸°ìˆ ì— ëŒ€í•œ íˆ¬ìë¥¼ ê³ ë ¤í•´ì•¼ í•˜ë©°, ê´€ë ¨ ì¸ì¬ë¥¼ í™•ë³´í•˜ê³  íŒŒíŠ¸ë„ˆì‹­ì„ ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤.
"""

REVIEW_STRONG_REPORT = """
AI ë³´ê³ ì„œ ìš”ì•½ (ìš°ìˆ˜ ì˜ˆì‹œ - ì ìˆ˜ 8.9/10)

ì‹œì¥ ì „ë§
AI ì‹œì¥ì€ 2024ë…„ 3.78ì–µ ë‹¬ëŸ¬ì—ì„œ 2034ë…„ 67.91ì–µ ë‹¬ëŸ¬ë¡œ ì„±ì¥í•  ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤(CAGR 33.49%, Market.us 2024). 
- ë¶ë¯¸ ì‹œì¥ì´ ì „ì²´ì˜ 42%ë¥¼ ì°¨ì§€í•˜ë©° ì„ ë„ì  ìœ„ì¹˜
- ì•„ì‹œì•„íƒœí‰ì–‘ ì§€ì—­ì€ ê°€ì¥ ë¹ ë¥¸ ì„±ì¥ë¥ (CAGR 38%)ì„ ê¸°ë¡ ì˜ˆìƒ
- ì£¼ìš” ì„±ì¥ ë™ë ¥: ìë™í™” ìˆ˜ìš” ì¦ê°€, AI ì¹© ì„±ëŠ¥ í–¥ìƒ, í´ë¼ìš°ë“œ ë¡œë³´í‹±ìŠ¤ ë°œì „

ê¸°ìˆ  íŠ¸ë Œë“œ
1. Vision-Language-Action (VLA) ëª¨ë¸: Googleì˜ RT-2, 1Xì˜ EVE ë“± ë©€í‹°ëª¨ë‹¬ foundation modelì´ ë¡œë´‡ì˜ ë²”ìš© ì‘ì—… ìˆ˜í–‰ ëŠ¥ë ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒ
2. World Foundation Models: NVIDIA Cosmos, Google Genesis ë“± ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°
3. ì—£ì§€ AI ê°€ì†í™”: Qualcomm RB6, NVIDIA Jetson Orinì„ í†µí•œ ì˜¨ë””ë°”ì´ìŠ¤ ì¶”ë¡ ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°˜ì‘ì„± 30% ê°œì„ 

ì‚°ì—…ë³„ ì ìš© ì‚¬ë¡€
ì œì¡°: Figure AIì™€ BMW í˜‘ë ¥ ì‚¬ë¡€ - South Carolina ê³µì¥ì—ì„œ Figure 02 íœ´ë¨¸ë…¸ì´ë“œê°€ ë¶€í’ˆ ì¡°ë¦½ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©° ìƒì‚°ì„± 25% í–¥ìƒ, í’ˆì§ˆ ë¶ˆëŸ‰ë¥  15% ê°ì†Œ (2024ë…„ 3ë¶„ê¸° ì‹¤ì¦ ê²°ê³¼)

ë¬¼ë¥˜: Amazonì´ 2024ë…„ Proteus AMR(ììœ¨ì£¼í–‰ ë¡œë´‡) 1,000ëŒ€ë¥¼ ë°°í¬í•˜ì—¬ ë¬¼ë¥˜ì„¼í„° ì²˜ë¦¬ëŸ‰ 40% ì¦ê°€ ë‹¬ì„±

í—¬ìŠ¤ì¼€ì–´: Diligent Roboticsì˜ Moxiê°€ ë¯¸êµ­ ë‚´ 200ê°œ ì´ìƒ ë³‘ì›ì—ì„œ ê°„í˜¸ì‚¬ ì—…ë¬´ ë¶€ë‹´ì„ 30% ê²½ê° (ì•½ë¬¼/ë¬¼í’ˆ ë°°ì†¡ ìë™í™”)

ê¶Œê³ ì‚¬í•­
1. ë‹¨ê³„ë³„ íˆ¬ì ë¡œë“œë§µ:
   - Phase 1 (6-12ê°œì›”): RPA ê¸°ë°˜ ë‹¨ìˆœ ì‘ì—… ìë™í™”ë¡œ ROI ê²€ì¦ (ì˜ˆìƒ íˆ¬ì: $50K-200K)
   - Phase 2 (1-2ë…„): AMR/í˜‘ë™ë¡œë´‡ ë„ì…ìœ¼ë¡œ ë¬¼ë¥˜/ì¡°ë¦½ ê³µì • ìµœì í™” (ì˜ˆìƒ íˆ¬ì: $500K-2M)
   - Phase 3 (2-3ë…„): AI ê¸°ë°˜ ì˜ì‚¬ê²°ì • ë° íœ´ë¨¸ë…¸ì´ë“œ íŒŒì¼ëŸ¿ (ì˜ˆìƒ íˆ¬ì: $2M+)

2. í•µì‹¬ íŒŒíŠ¸ë„ˆì‹­ ì „ëµ:
   - í•˜ë“œì›¨ì–´: Boston Dynamics, Agility Roboticsì™€ POC ê³„ì•½ ì²´ê²°
   - ì†Œí”„íŠ¸ì›¨ì–´: OpenAI, Covariant ë“± AI í”Œë«í¼ ë²¤ë”ì™€ ë¼ì´ì„ ìŠ¤ í˜‘ìƒ
   - ì‹œìŠ¤í…œ í†µí•©: Accenture, Deloitteì˜ SI ì»¨ì„¤íŒ… í™œìš©

3. ì¸ì¬ í™•ë³´ ê³„íš:
   - Robotics Engineer 2-3ëª…, ML Engineer 3-5ëª… ì±„ìš© (ì—°ë´‰ ë²”ìœ„: $120K-180K)
   - UC Berkeley, CMU ë“± ì£¼ìš” ëŒ€í•™ ì—°êµ¬ì‹¤ê³¼ ì¸í„´ì‹­ í”„ë¡œê·¸ë¨ ìš´ì˜

ì°¸ê³  ìë£Œ ë° ì¶œì²˜
- Market.us, "Physical AI Market Forecast to 2034", 2024
URL: https://market.us/report/physical-ai-market
- Google Research, "RT-2: Vision-Language-Action Model for Robotics"
URL: https://ai.googleblog.com/2024/02/rt-2-vision-language-action-model-for.html
- NVIDIA, "Cosmos: AI-Driven Simulation for Robotics", 2024
URL: https://developer.nvidia.com/blog/cosmos-ai-driven-simulation-for-robotics/
"""

REVIEW_FEW_SHOT_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ AI ì „ë¬¸ ê°ìˆ˜ìì…ë‹ˆë‹¤. ì•„ë˜ 5ê°€ì§€ ê¸°ì¤€(ê° 20ì , ì´ 100ì )ìœ¼ë¡œ í‰ê°€í•˜ê³ , 0-10ì  ì‚¬ì´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:

A. ë‚´ìš© ì™„ì„±ë„ (20ì )
- 18-20ì : ëª¨ë“  í•µì‹¬ ì§ˆë¬¸ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ë©° ì¶”ê°€ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- 14-17ì : ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì„ ì¶©ì‹¤íˆ ë‹µí•˜ë‚˜ ì¼ë¶€ ì˜ì—­ì—ì„œ ê¹Šì´ ë¶€ì¡±
- 10-13ì : ê¸°ë³¸ì ì¸ ë‚´ìš©ì€ í¬í•¨í•˜ë‚˜ í‘œë©´ì  ìˆ˜ì¤€
- 0-9ì : ì£¼ìš” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ê±°ë‚˜ ë‚´ìš©ì´ ë¶ˆì¶©ë¶„

B. ë°ì´í„° ì •í™•ì„± (20ì )
- 18-20ì : ê²€ì¦ ê°€ëŠ¥í•œ ìµœì‹  ë°ì´í„°ì™€ ëª…í™•í•œ ì¶œì²˜ ì œì‹œ
- 14-17ì : ëŒ€ì²´ë¡œ ì •í™•í•˜ë‚˜ ì¼ë¶€ ì¶œì²˜ ë¯¸ë¹„
- 10-13ì : ì •ì„±ì  ì„œìˆ  ìœ„ì£¼, ìˆ˜ì¹˜ ê·¼ê±° ë¶€ì¡±
- 0-9ì : ë¶€ì •í™•í•œ ì •ë³´ í¬í•¨ ë˜ëŠ” ì¶œì²˜ ì—†ìŒ

C. êµ¬ì¡° ë…¼ë¦¬ì„± (20ì )
- 18-20ì : ë§¤ë„ëŸ¬ìš´ ì „ê°œì™€ ëª…í™•í•œ ë…¼ë¦¬ì  ì—°ê²°
- 14-17ì : ì „ë°˜ì ìœ¼ë¡œ ì²´ê³„ì ì´ë‚˜ ì¼ë¶€ ì¤‘ë³µ/ë¹„ì•½
- 10-13ì : êµ¬ì¡°ëŠ” ìˆìœ¼ë‚˜ ì„¹ì…˜ ê°„ ì—°ê²° ì•½í•¨
- 0-9ì : ë¹„ì²´ê³„ì ì´ê±°ë‚˜ ë…¼ë¦¬ì  íë¦„ ë¶€ì¬

D. ì‹¤í–‰ ê°€ëŠ¥ì„± (20ì )
- 18-20ì : ì¬ë¬´/ê¸°ìˆ /íŒŒíŠ¸ë„ˆì‹­ ë“± êµ¬ì²´ì  ì‹¤í–‰ ì§€ì¹¨ ì œê³µ
- 14-17ì : ë°©í–¥ì„±ì€ ëª…í™•í•˜ë‚˜ ì„¸ë¶€ ë‹¨ê³„ ë¶€ì¡±
- 10-13ì : ì¶”ìƒì  ê¶Œê³ ì•ˆ ìˆ˜ì¤€
- 0-9ì : ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ ì—†ìŒ

E. ì „ë¬¸ì„± (20ì )
- 18-20ì : ì—…ê³„ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í†µì°°ê³¼ ìš©ì–´ í™œìš©
- 14-17ì : ê¸°ë³¸ì ì¸ ì „ë¬¸ì„±ì€ ê°–ì¶”ì—ˆìœ¼ë‚˜ ì°¨ë³„í™” ë¶€ì¡±
- 10-13ì : ì¼ë°˜ì  ìˆ˜ì¤€ì˜ ë¶„ì„
- 0-9ì : ì „ë¬¸ì„± ê²°ì—¬

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: x.x/10 (ì´ì : yy/100)

ì„¸ë¶€ í‰ê°€:
- A. ë‚´ìš© ì™„ì„±ë„: xx/20ì  - ...
- B. ë°ì´í„° ì •í™•ì„±: xx/20ì  - ...
- C. êµ¬ì¡° ë…¼ë¦¬ì„±: xx/20ì  - ...
- D. ì‹¤í–‰ ê°€ëŠ¥ì„±: xx/20ì  - ...
- E. ì „ë¬¸ì„±: xx/20ì  - ..."""),
    
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ (ë¯¸í¡ ì˜ˆì‹œ):
{baseline_report}"""),
    
    ("assistant", """ì ìˆ˜: 5.8/10 (ì´ì : 58/100)

ì„¸ë¶€ í‰ê°€:
- A. ë‚´ìš© ì™„ì„±ë„: 11/20ì  - í•µì‹¬ ì˜ì—­(ì‹œì¥, ê¸°ìˆ , ì‚°ì—…, ê¶Œê³ )ì„ ì–¸ê¸‰í–ˆìœ¼ë‚˜ ê° ì˜ì—­ì˜ ê¹Šì´ê°€ ë§¤ìš° ë¶€ì¡±í•˜ê³  í‘œë©´ì ì…ë‹ˆë‹¤. "ë¹ ë¥´ê²Œ ì„±ì¥"ê³¼ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ë§Œ ìˆê³  êµ¬ì²´ì ì¸ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.

- B. ë°ì´í„° ì •í™•ì„±: 8/20ì  - ë‹¨ í•˜ë‚˜ì˜ ìˆ˜ì¹˜ë‚˜ ì¶œì²˜ë„ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "ë§ì€ ê¸°ì—…ë“¤", "ë‹¤ì–‘í•œ ì‚°ì—…" ë“± ê²€ì¦ ë¶ˆê°€ëŠ¥í•œ ì •ì„±ì  ì„œìˆ ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì´ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.

- C. êµ¬ì¡° ë…¼ë¦¬ì„±: 12/20ì  - ê¸°ë³¸ì ì¸ ì„¹ì…˜ êµ¬ë¶„ì€ ìˆìœ¼ë‚˜ ê° ì„¹ì…˜ì´ 2-3ì¤„ë¡œ ë§¤ìš° ì§§ê³ , ì‹œì¥ ì „ë§ê³¼ ê¸°ìˆ  íŠ¸ë Œë“œ, ì‚°ì—… ì ìš©ì´ ì„œë¡œ ì—°ê²°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš”ì•½-ë³¸ë¬¸-ê²°ë¡ ì˜ êµ¬ì¡°ê°€ ì—†ìŠµë‹ˆë‹¤.

- D. ì‹¤í–‰ ê°€ëŠ¥ì„±: 10/20ì  - ê¶Œê³ ì‚¬í•­ì´ "íˆ¬ìë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤", "ì¸ì¬ë¥¼ í™•ë³´í•´ì•¼ í•œë‹¤"ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ë‹¹ìœ„ì„±ë§Œ ì œì‹œí•©ë‹ˆë‹¤. íˆ¬ì ê·œëª¨, ì‹œê¸°, ìš°ì„ ìˆœìœ„, êµ¬ì²´ì  íŒŒíŠ¸ë„ˆ í›„ë³´, ì±„ìš© ì§ë¬´/ì¸ì› ë“± ì‹¤ë¬´ ì§€ì¹¨ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤.

- E. ì „ë¬¸ì„±: 17/20ì  - "AI ê¸°ìˆ ", "ë¡œë´‡ ê¸°ìˆ ", "ì„¼ì„œ ê¸°ìˆ "ì²˜ëŸ¼ ì¼ë°˜ì¸ë„ ì•„ëŠ” ìˆ˜ì¤€ì˜ ìš©ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. VLA, AMR, RPA, íœ´ë¨¸ë…¸ì´ë“œ ë“± ì—…ê³„ ì „ë¬¸ ìš©ì–´ê°€ ì—†ê³ , í†µì°°ì´ë‚˜ íŠ¸ë Œë“œ ë¶„ì„ì´ ê²°ì—¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤."""),
    
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ (ìš°ìˆ˜ ì˜ˆì‹œ):
{strong_report}"""),
    
    ("assistant", """ì ìˆ˜: 8.9/10 (ì´ì : 89/100)

ì„¸ë¶€ í‰ê°€:
- A. ë‚´ìš© ì™„ì„±ë„: 18/20ì  - ì‹œì¥, ê¸°ìˆ , ì‚°ì—…, ê¶Œê³  ëª¨ë“  ì˜ì—­ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì‹œì¥ ì„¹ì…˜ì€ ê¸ˆì•¡/ì„±ì¥ë¥ /ì§€ì—­ë³„ ë¶„ì„, ê¸°ìˆ  ì„¹ì…˜ì€ 3ê°€ì§€ ì£¼ìš” íŠ¸ë Œë“œì™€ ì‹¤ì œ ì œí’ˆ, ì‚°ì—… ì„¹ì…˜ì€ 3ê°œ ì‚°ì—…ë³„ ì •ëŸ‰ ì„±ê³¼ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. 2ì  ê°ì  ì‚¬ìœ ëŠ” ë„ì „ê³¼ì œ/ìœ„í—˜ ê´€ë¦¬ ì˜ì—­ì´ ìƒëŒ€ì ìœ¼ë¡œ ì•½í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

- B. ë°ì´í„° ì •í™•ì„±: 17/20ì  - ë‹¤ìˆ˜ì˜ ìˆ˜ì¹˜(ì‹œì¥ ê·œëª¨ $67.91B, CAGR 33.49%, ìƒì‚°ì„± 25% í–¥ìƒ, ë¬¼ë¥˜ ì²˜ë¦¬ëŸ‰ 40% ì¦ê°€)ì™€ êµ¬ì²´ì ì¸ ì‚¬ë¡€(BMW ê³µì¥, Amazon AMR 1,000ëŒ€)ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì¶œì²˜ëŠ” ì¼ë¶€ ëª…ì‹œ(Market.us 2024)í–ˆìœ¼ë‚˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ ì¶œì²˜ê°€ í•„ìš”í•˜ê³ , ì°¸ê³ ë¬¸í—Œ ì „ì²´ ëª©ë¡ì´ ì—†ì–´ 3ì  ê°ì í–ˆìŠµë‹ˆë‹¤.

- C. êµ¬ì¡° ë…¼ë¦¬ì„±: 18/20ì  - ì‹œì¥ ì „ë§ â†’ ê¸°ìˆ  ê¸°ë°˜ â†’ ì‚°ì—… ì ìš© â†’ ì‹¤í–‰ ê¶Œê³ ë¡œ ì´ì–´ì§€ëŠ” ë…¼ë¦¬ì  íë¦„ì´ ë§¤ë„ëŸ½ìŠµë‹ˆë‹¤. ê° ì„¹ì…˜ì´ ë‹¤ìŒ ì„¹ì…˜ì˜ ê·¼ê±°ê°€ ë˜ë©°, ê¶Œê³ ì‚¬í•­ì´ ì•ì„œ ì œì‹œí•œ ì‹œì¥/ê¸°ìˆ  ë¶„ì„ê³¼ ì •ë ¬ë©ë‹ˆë‹¤. 2ì  ê°ì  ì‚¬ìœ ëŠ” ê²°ë¡ /ìš”ì•½ ì„¹ì…˜ì´ ì—†ì–´ ì „ì²´ë¥¼ ì•„ìš°ë¥´ëŠ” ë§ˆë¬´ë¦¬ê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

- D. ì‹¤í–‰ ê°€ëŠ¥ì„±: 19/20ì  - 3ë‹¨ê³„ íˆ¬ì ë¡œë“œë§µì— ê° ë‹¨ê³„ë³„ ê¸°ê°„, íˆ¬ì ê·œëª¨($50K-$2M+), êµ¬ì²´ì  ê¸°ìˆ (RPA, AMR, íœ´ë¨¸ë…¸ì´ë“œ), íŒŒíŠ¸ë„ˆ í›„ë³´(Boston Dynamics, OpenAI, Accenture), ì±„ìš© ì¸ì›(2-3ëª…, 3-5ëª…)ê³¼ ì—°ë´‰($120K-$180K)ê¹Œì§€ ì œì‹œí•˜ì—¬ ì¦‰ì‹œ ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. 1ì  ê°ì  ì‚¬ìœ ëŠ” ê° ë‹¨ê³„ì˜ ì„±ê³µ/ì‹¤íŒ¨ ê¸°ì¤€(KPI)ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

- E. ì „ë¬¸ì„±: 17/20ì  - VLA, World Foundation Models, AMR, POC, SI ë“± ì—…ê³„ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ê³ , RT-2, EVE, Cosmos, Proteus ë“± ìµœì‹  ì œí’ˆëª…ì„ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. UC Berkeley, CMU ê°™ì€ ì—°êµ¬ ê¸°ê´€ê¹Œì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì—¬ ë„ë©”ì¸ ì§€ì‹ì´ í’ë¶€í•©ë‹ˆë‹¤. 3ì  ê°ì  ì‚¬ìœ ëŠ” ì¼ë¶€ ì•½ì–´(AMR, RPA)ì— ëŒ€í•œ ì²« ì–¸ê¸‰ ì‹œ í’€ë„¤ì„ ë³‘ê¸°ê°€ ì—†ê³ , ê¸°ìˆ ì  ê¹Šì´(ì˜ˆ: VLA ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜)ê°€ ë” ìˆìœ¼ë©´ ì „ë¬¸ì„±ì´ ë†’ì•„ì§ˆ ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."""),
    
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ:
{report}

ìœ„ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ì„¸ìš”."""),
])

# ==================== Tavily ê²€ìƒ‰ ì„¤ì • ====================
def get_tavily_search(max_results: int = 5):
    """Tavily ê²€ìƒ‰ ë„êµ¬ ìƒì„±"""
    return TavilySearchResults(
        max_results=max_results,
        search_depth="advanced",  # "basic" or "advanced"
        include_answer=True,  # AI ìƒì„± ë‹µë³€ í¬í•¨
        include_raw_content=False,  # only summary text to keep logs small
        include_images=False
    )

# ==================== ìƒíƒœ ë³‘í•© ìœ í‹¸ë¦¬í‹° ====================
def preserve_user_query(existing: Optional[str], new: str) -> str:
    """Keep the first user query value when multiple nodes emit it."""
    return existing if existing else new

def replace_research_plan(existing: Optional[Dict[str, List[str]]], new: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """Prefer the newest non-empty research plan."""
    if new:
        return new
    return existing or {}

def replace_result_list(existing: Optional[List[Dict]], new: List[Dict]) -> List[Dict]:
    """Prefer the latest non-empty research results list."""
    if new:
        return new
    return existing or []

def merge_search_context(existing: Optional[Dict[str, str]], new: Dict[str, str]) -> Dict[str, str]:
    """Merge search context dictionaries across parallel nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged

def merge_report_sections(existing: Optional[Dict[str, str]], new: Dict[str, str]) -> Dict[str, str]:
    """Merge report section text updates across nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged


def merge_synthesized_data(existing: Optional[Dict[str, Any]], new: Dict[str, Any]) -> Dict[str, Any]:
    """Merge synthesized insights coming from multiple nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged

def replace_final_report(existing: Optional[str], new: str) -> str:
    """Prefer the latest final report content."""
    return new if new else (existing or "")

def replace_quality_score(existing: Optional[float], new: float) -> float:
    """Prefer the latest quality score provided."""
    return new if new is not None else (existing if existing is not None else 0.0)

def replace_iteration_count(existing: Optional[int], new: int) -> int:
    """Keep the highest iteration count emitted."""
    existing_val = existing if existing is not None else 0
    new_val = new if new is not None else 0
    return max(existing_val, new_val)

# ==================== State ì •ì˜ ====================
class AgentState(TypedDict):
    """ì—ì´ì „íŠ¸ ìƒíƒœ ê´€ë¦¬"""
    user_query: Annotated[str, preserve_user_query]
    research_plan: Annotated[Dict[str, List[str]], replace_research_plan]
    market_data: Annotated[List[Dict], replace_result_list]
    tech_data: Annotated[List[Dict], replace_result_list]
    industry_data: Annotated[List[Dict], replace_result_list]
    company_data: Annotated[List[Dict], replace_result_list]
    challenge_data: Annotated[List[Dict], replace_result_list]
    synthesized_data: Annotated[Dict[str, Any], merge_synthesized_data]
    report_sections: Annotated[Dict[str, str], merge_report_sections]
    final_report: Annotated[str, replace_final_report]
    quality_score: Annotated[float, replace_quality_score]
    iteration_count: Annotated[int, replace_iteration_count]
    search_context: Annotated[Dict[str, str], merge_search_context]  # Tavily ë‹µë³€ ì €ì¥
    messages: Annotated[List[str], operator.add]


def execute_tavily_query(tavily_search, query: str) -> Dict[str, Any]:
    """Run Tavily with fallbacks to avoid empty responses."""
    # âœ… ë‹¨ìˆœí™”ëœ ì‹œë„ ëª©ë¡ (ë¬¸ìì—´ë§Œ)
    attempts = [
        query,  # ì›ë³¸ ì¿¼ë¦¬
        f"{query} 2024 2025",  # ì—°ë„ ì¶”ê°€
        f"{query} analysis",  # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
    ]
    last_error = ""

    for idx, query_str in enumerate(attempts, 1):
        try:
            print(f"ğŸ” ì‹œë„ {idx}: {query_str[:50]}...")
            result = tavily_search.invoke(query_str)  # âœ… ë¬¸ìì—´ ì§ì ‘ ì „ë‹¬
            #print(f"âœ… ì„±ê³µ! ê²°ê³¼ íƒ€ì…: {type(result)}")
            
            # ê²°ê³¼ ì²˜ë¦¬
            if isinstance(result, dict):
                if result.get("results"):
                    result.setdefault("answer", "")
                    result.setdefault("error", "")
                    return result
                last_error = result.get("error", "no results")
                print(f"âš ï¸ ë¹ˆ ê²°ê³¼: {last_error}")
                continue

            if isinstance(result, list):
                if result:
                    return {"results": result, "answer": "", "error": ""}
                print(f"âš ï¸ ë¹ˆ ë¦¬ìŠ¤íŠ¸")
                continue
                
        except Exception as exc:
            last_error = str(exc)
            print(f"âŒ ì—ëŸ¬: {exc}")
            
            # 432 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
            if "432" in str(exc):
                print("\nâš ï¸ Tavily API 432 ì—ëŸ¬ ë°œìƒ!")
                print("ê°€ëŠ¥í•œ ì›ì¸:")
                print("1. API í‚¤ê°€ ë¬´íš¨í•˜ê±°ë‚˜ ë§Œë£Œë¨")
                print("2. ë¬´ë£Œ í”Œëœ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ (https://app.tavily.com/home ì—ì„œ í™•ì¸)")
                print("3. API í‚¤ ê¶Œí•œ ë¬¸ì œ")
                print("\ní•´ê²° ë°©ë²•:")
                print("- ìƒˆ API í‚¤ ë°œê¸‰: https://app.tavily.com/")
                print("- í™˜ê²½ë³€ìˆ˜ ì¬ì„¤ì •: export TAVILY_API_KEY='your-new-key'")
                break  # 432 ì—ëŸ¬ëŠ” ì¬ì‹œë„ ë¶ˆí•„ìš”
            
            continue

    return {"results": [], "answer": "", "error": last_error or "empty Tavily response"}

def strengthen_keyword(category: str, keyword: str, state: AgentState) -> str:
    """Adjust Tavily query with quality feedback to target weak metrics."""
    feedback = state.get("search_context", {}).get("quality_feedback", {})
    issues = feedback.get(category, [])
    if not issues:
        return keyword

    modifiers: List[str] = []
    if "items" in issues or "coverage" in issues:
        modifiers.append("comprehensive data reliable sources 2024")
    if "answers" in issues:
        modifiers.append("key insights summary")
    if "content" in issues:
        modifiers.append("detailed report in depth analysis")
    if "score" in issues:
        modifiers.append("official statistics verified figures")

    if modifiers:
        boost = " ".join(modifiers)
        if boost not in keyword:
            return f"{keyword} {boost}"
    return keyword
# ==================== ë…¸ë“œ í•¨ìˆ˜ë“¤ ====================

def planning_node(state: AgentState) -> AgentState:
    """ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½"""
    state.setdefault("search_context", {})
    try:
        plan = planning_chain.invoke({
            "query": state["user_query"]
        })
        state["research_plan"] = {
            "market": list(plan.market.search_keywords),
            "tech": list(plan.tech.search_keywords),
            "industry": list(plan.industry.search_keywords),
            "company": list(plan.company.search_keywords),
            "challenge": list(plan.challenge.search_keywords)
        }
        state["search_context"]["plan_overview"] = plan.plan_overview
        for area in ("market", "tech", "industry", "company", "challenge"):
            area_plan = getattr(plan, area)
            state["search_context"][f"{area}_focus"] = area_plan.focus_question
            print(f"ğŸ”– {area} í•µì‹¬ ì§ˆë¬¸: {area_plan.focus_question}")
        state["messages"].append("âœ… LLM ê¸°ë°˜ ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ")
        print("âœ… ë¦¬ì„œì¹˜ ê³„íš: ìˆ˜ë¦½ì™„ë£Œ")
        overview = plan.plan_overview.strip()
        if overview:
            state["messages"].append(f"ğŸ§­ ê³„íš ìš”ì•½: {overview}")
        print(f"ğŸ§­ ê³„íš ìš”ì•½: {overview}")
    except Exception as exc:
        print(f"ë¦¬ì„œì¹˜ ê³„íš ìƒì„± ì˜¤ë¥˜: {exc}")
        state["research_plan"] = {category: list(keywords) for category, keywords in DEFAULT_RESEARCH_PLAN.items()}
        state["search_context"]["plan_overview"] = "ì‚¬ì „ ì •ì˜ëœ ê¸°ë³¸ ë¦¬ì„œì¹˜ ê³„íš ì‚¬ìš©"
        state["messages"].append("âš ï¸ ê¸°ë³¸ ë¦¬ì„œì¹˜ ê³„íšìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤")
    return state

def market_research_node(state: AgentState) -> AgentState:
    """ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    market_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["market"]:
        try:
            # Tavily ê²€ìƒ‰ ì‹¤í–‰
            query = strengthen_keyword("market", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            # ê²°ê³¼ ì²˜ë¦¬ (TavilyëŠ” êµ¬ì¡°í™”ëœ Dict ë°˜í™˜)
            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],  # 500ì ì œí•œ
                    "score": result.get("score", 0.0)  # ê´€ë ¨ì„± ì ìˆ˜
                })
            
            market_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value  # Tavily AI ë‹µë³€
            })
            
            # AI ë‹µë³€ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            market_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["market_data"] = market_data
    
    # search_context ì´ˆê¸°í™” (ì²˜ìŒ ì‹¤í–‰ì‹œ)
    if "search_context" not in state:
        state["search_context"] = {}
    state["search_context"]["market"] = "\n".join(search_contexts)
    
    state["messages"].append(f"âœ… ì‹œì¥ ë°ì´í„° {len(market_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def tech_research_node(state: AgentState) -> AgentState:
    """ê¸°ìˆ  ë™í–¥ ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    tech_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["tech"]:
        try:
            query = strengthen_keyword("tech", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            tech_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            tech_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["tech_data"] = tech_data
    state["search_context"]["tech"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ê¸°ìˆ  ë°ì´í„° {len(tech_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def industry_research_node(state: AgentState) -> AgentState:
    """ì‚°ì—…ë³„ ì‚¬ë¡€ ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    industry_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["industry"]:
        try:
            query = strengthen_keyword("industry", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            industry_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            industry_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["industry_data"] = industry_data
    state["search_context"]["industry"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ì‚°ì—… ë°ì´í„° {len(industry_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def company_research_node(state: AgentState) -> AgentState:
    """ê¸°ì—… ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    company_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["company"]:
        try:
            query = strengthen_keyword("company", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            company_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            company_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["company_data"] = company_data
    state["search_context"]["company"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ê¸°ì—… ë°ì´í„° {len(company_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def challenge_research_node(state: AgentState) -> AgentState:
    """ë„ì „ê³¼ì œ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    challenge_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["challenge"]:
        try:
            query = strengthen_keyword("challenge", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            challenge_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            challenge_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["challenge_data"] = challenge_data
    state["search_context"]["challenge"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ë„ì „ê³¼ì œ ë°ì´í„° {len(challenge_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def synthesis_node(state: AgentState) -> AgentState:
    """ìˆ˜ì§‘ëœ ë°ì´í„° í†µí•© ë° ë¶„ì„ (Tavily ë‹µë³€ í™œìš©)"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    # Tavily AI ë‹µë³€ì„ ìš°ì„  í™œìš©
    all_data = {
        "market": {
            "tavily_answers": state["search_context"].get("market", ""),
            "detailed_data": state["market_data"]
        },
        "tech": {
            "tavily_answers": state["search_context"].get("tech", ""),
            "detailed_data": state["tech_data"]
        },
        "industry": {
            "tavily_answers": state["search_context"].get("industry", ""),
            "detailed_data": state["industry_data"]
        },
        "company": {
            "tavily_answers": state["search_context"].get("company", ""),
            "detailed_data": state["company_data"]
        },
        "challenge": {
            "tavily_answers": state["search_context"].get("challenge", ""),
            "detailed_data": state["challenge_data"]
        }
    }
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ AI ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        Tavily AIê°€ ì œê³µí•œ ë‹µë³€ê³¼ ìƒì„¸ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
        ê° ì˜ì—­ë³„ë¡œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¸ë Œë“œ 3-5ê°€ì§€ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”."""),
        ("user", """ì¹´í…Œê³ ë¦¬: {category}
        
Tavily AI ë‹µë³€:
{tavily_answers}

ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ ê²°ê³¼):
{detailed_data}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.""")
    ])
    
    synthesized = {}
    for category, data in all_data.items():
        # ìƒì„¸ ë°ì´í„° ìš”ì•½ (ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ)
        detailed_summary = []
        for item in data["detailed_data"][:3]:
            if item.get("results"):
                top_result = item["results"][0]
                detailed_summary.append(
                    f"- {top_result.get('title', '')}: {top_result.get('content', '')[:200]}"
                )
        
        response = llm.invoke(
            prompt.format_messages(
                category=category,
                tavily_answers=data["tavily_answers"],
                detailed_data="\n".join(detailed_summary)
            )
        )
        synthesized[category] = response.content
    
    state["synthesized_data"] = synthesized
    state["messages"].append("âœ… ë°ì´í„° í†µí•© ë¶„ì„ ì™„ë£Œ (Tavily AI ë‹µë³€ í™œìš©)")
    return state

def quality_check_node(state: AgentState) -> str:
    """   ( )"""
    state.setdefault("search_context", {})
    categories = ["market", "tech", "industry", "company", "challenge"]
    diagnostics: List[str] = []
    failing: List[str] = []
    quality_feedback: Dict[str, List[str]] = {}

    for category in categories:
        data_key = f"{category}_data"
        entries = state.get(data_key, []) or []
        print(f"í’ˆì§ˆ ê²€ì‚¬ - {category}: {len(entries)} í•­ëª© ë¶„ì„ ì¤‘...")
        print(entries[:2])  # ì²˜ìŒ 2ê°œ í•­ëª© ì¶œë ¥
        total = len(entries)
        hits = sum(1 for item in entries if item.get("results"))
        answers = sum(1 for item in entries if item.get("answer"))

        failing_metrics: List[str] = []
        if total == 0:
            failing_metrics.append("items")
        if hits == 0:
            failing_metrics.append("coverage")
        if answers == 0:
            failing_metrics.append("answers")

        passes = total > 0 and (hits > 0 or answers > 0)

        if failing_metrics:
            quality_feedback[category] = failing_metrics

        diagnostics.append(
            f"{category}: total={total}, hits={hits}, answers={answers} -> {'PASS' if passes else 'RECHECK'}"
        )
        if not passes:
            failing.append(category)

    state["search_context"]["quality_feedback"] = quality_feedback
    state["search_context"]["quality_diagnostics"] = "\n".join(diagnostics)
    state["messages"].append("     - " + " | ".join(diagnostics))

    current_loop = state.get("iteration_count", 0) + 1
    
    state["iteration_count"] = current_loop 
    max_loops = 2
    
    if failing and current_loop < max_loops:
        print(f"í˜„ì¬ ë°˜ë³µ íšŸìˆ˜: {current_loop}")
        state["messages"].append(f"  : {', '.join(failing)} ( )")
        return "research_more"

    if failing:
        state["messages"].append(f"        : {', '.join(failing)} (  {max_loops}  )")

    state["messages"].append("    ")
    state["iteration_count"] = 0
    return "generate_report"

def report_generation_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± - ê³µì‹ì ì´ê³  ì •í˜•í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ ì–‘ì‹"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    sections = {
        "executive_summary": "í•µì‹¬ ìš”ì•½",
        "market_overview": "ì‹œì¥ ì „ë§",
        "technology_trends": "ê¸°ìˆ  íŠ¸ë Œë“œ",
        "industry_applications": "ì‚°ì—…ë³„ ì‘ìš©",
        "key_players": "ì£¼ìš” ê¸°ì—…",
        "challenges": "ë„ì „ê³¼ì œ",
        "forecast": "í–¥í›„ 5ë…„ ì „ë§",
        "recommendations": "ì „ëµì  ê¶Œê³ ì‚¬í•­",
        "conclusion": "ê²°ë¡ "
    }

    # ì„¹ì…˜ë³„ ìƒì„¸ ì‘ì„± ê°€ì´ë“œë¼ì¸
    section_guidelines = {
        "executive_summary": """
        - ë³´ê³ ì„œ ì „ì²´ì˜ í•µì‹¬ ë‚´ìš©ì„ 3-5ê°œì˜ ì£¼ìš” í¬ì¸íŠ¸ë¡œ ìš”ì•½
        - ì‹œì¥ ê·œëª¨, ì„±ì¥ë¥ (CAGR) ë“± í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ë°˜ë“œì‹œ í¬í•¨
        - ì£¼ìš” íŠ¸ë Œë“œì™€ ê¸°ìˆ  í˜ì‹ ì„ ê°„ëµíˆ ì–¸ê¸‰
        - ì „ëµì  ì‹œì‚¬ì ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œì‹œ
        """,
        "market_overview": """
        - í˜„ì¬ ì‹œì¥ ê·œëª¨ì™€ ì˜ˆì¸¡ ì‹œì¥ ê·œëª¨ë¥¼ êµ¬ì²´ì ì¸ ê¸ˆì•¡($)ìœ¼ë¡œ ì œì‹œ
        - CAGR(ì—°í‰ê·  ì„±ì¥ë¥ ) ìˆ˜ì¹˜ì™€ ê¸°ê°„ ëª…ì‹œ
        - ì§€ì—­ë³„ ì‹œì¥ ë¹„ì¤‘(ë¶ë¯¸, ìœ ëŸ½, ì•„ì‹œì•„íƒœí‰ì–‘ ë“±) ë°±ë¶„ìœ¨ë¡œ ì œì‹œ
        - ì£¼ìš” ì„±ì¥ ë™ë ¥(ì˜ˆ: ìë™í™” ìˆ˜ìš”, AI ì¹© ë°œì „ ë“±)ì„ 3-4ê°€ì§€ ë‚˜ì—´
        - ê°€ëŠ¥í•œ ê²½ìš° ì¶œì²˜(ì˜ˆ: Market.us 2024, IDC 2025) ëª…ì‹œ
        """,
        "technology_trends": """
        - í•µì‹¬ ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ 3-5ê°œ ì„ ì •í•˜ì—¬ ê°ê° ìƒì„¸ ì„¤ëª…
        - ê° ê¸°ìˆ ë³„ë¡œ ì‹¤ì œ ì œí’ˆëª…ì´ë‚˜ í”„ë¡œì íŠ¸ëª… ì–¸ê¸‰(ì˜ˆ: Google RT-2, NVIDIA Cosmos)
        - ê¸°ìˆ ì  ì„±ëŠ¥ ê°œì„  ìˆ˜ì¹˜ë¥¼ í¬í•¨(ì˜ˆ: ì •í™•ë„ 30% í–¥ìƒ, ì§€ì—°ì‹œê°„ 50ms ë‹¨ì¶•)
        - ì—…ê³„ ì „ë¬¸ ìš©ì–´ ì ê·¹ í™œìš©(VLA, AMR, Edge AI, Foundation Model ë“±)
        - ê° ê¸°ìˆ ì˜ ì‚°ì—…ì  ì˜ì˜ì™€ ì ìš© ê°€ëŠ¥ì„± ì„¤ëª…
        """,
        "industry_applications": """
        - ìµœì†Œ 3ê°œ ì´ìƒì˜ ì£¼ìš” ì‚°ì—… ë¶„ì•¼ ë‹¤ë£¨ê¸°(ì œì¡°, ë¬¼ë¥˜, í—¬ìŠ¤ì¼€ì–´, ììœ¨ì£¼í–‰ ë“±)
        - ê° ì‚°ì—…ë³„ë¡œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ ì œì‹œ(ê¸°ì—…ëª…, í”„ë¡œì íŠ¸ëª… í¬í•¨)
        - ì •ëŸ‰ì  ì„±ê³¼ ì§€í‘œ í¬í•¨(ì˜ˆ: ìƒì‚°ì„± 25% í–¥ìƒ, ë¬¼ë¥˜ ì²˜ë¦¬ëŸ‰ 40% ì¦ê°€)
        - ì‹¤ì¦ ë°ì´í„°ë‚˜ íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ ê²°ê³¼ ì–¸ê¸‰
        - ê° ì‚°ì—…ì—ì„œì˜ ë„ì… ë‹¨ê³„(ì´ˆê¸°/ì„±ì¥/ì„±ìˆ™) í‰ê°€
        """,
        "key_players": """
        - ì£¼ìš” ê¸°ì—… 5-7ê°œë¥¼ ì„ ì •í•˜ì—¬ ê°ê°ì˜ ì „ëµê³¼ ì œí’ˆ ì†Œê°œ
        - ìµœê·¼ íˆ¬ì ìœ ì¹˜ ê¸ˆì•¡, íŒŒíŠ¸ë„ˆì‹­, M&A ì •ë³´ í¬í•¨
        - ê° ê¸°ì—…ì˜ ê¸°ìˆ ì  ì°¨ë³„ì ê³¼ ì‹œì¥ í¬ì§€ì…”ë‹ ì„¤ëª…
        - ìŠ¤íƒ€íŠ¸ì—…ê³¼ ëŒ€ê¸°ì—…ì„ êµ¬ë¶„í•˜ì—¬ ë¶„ì„
        - ì£¼ìš” ì œí’ˆì˜ êµ¬ì²´ì ì¸ ìŠ¤í™ì´ë‚˜ ì„±ëŠ¥ ì§€í‘œ ì–¸ê¸‰
        """,
        "challenges": """
        - ê¸°ìˆ ì  ê³¼ì œ(ì˜ˆ: ë°°í„°ë¦¬, ì •ë°€ë„, ì•ˆì „ì„±) 3-4ê°€ì§€
        - ë¹„ì¦ˆë‹ˆìŠ¤ ì¥ë²½(ì˜ˆ: ë†’ì€ ì´ˆê¸° ë¹„ìš©, ROI ë¶ˆí™•ì‹¤ì„±) 2-3ê°€ì§€
        - ê·œì œ ë° ìœ¤ë¦¬ì  ì´ìŠˆ(ì˜ˆ: ì•ˆì „ ê·œì œ, ì¼ìë¦¬ ëŒ€ì²´) 1-2ê°€ì§€
        - ê° ê³¼ì œì— ëŒ€í•œ í˜„ì¬ í•´ê²° ì‹œë„ë‚˜ ëŒ€ì•ˆ ì–¸ê¸‰
        - í–¥í›„ í•´ê²° ì „ë§ê³¼ ì˜ˆìƒ ì‹œì  ì œì‹œ
        """,
        "forecast": """
        - í–¥í›„ 5ë…„ê°„ì˜ ë‹¨ê³„ì  ë°œì „ ì‹œë‚˜ë¦¬ì˜¤ ì œì‹œ
        - ì—°ë„ë³„ ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ ì˜ˆì¸¡(ì˜ˆ: 2026ë…„ ëŒ€ëŸ‰ ë°°í¬, 2028ë…„ í‘œì¤€í™”)
        - ì‹œì¥ ì¹¨íˆ¬ìœ¨ ì „ë§(ì˜ˆ: 2030ë…„ê¹Œì§€ ì œì¡°ì—…ì˜ 30% ë„ì…)
        - ê¸°ìˆ  ì„±ìˆ™ë„ ë¡œë“œë§µ(ì´ˆê¸°â†’ì„±ì¥â†’ì„±ìˆ™ ë‹¨ê³„)
        - ë‚™ê´€ì /ë³´ìˆ˜ì  ì‹œë‚˜ë¦¬ì˜¤ êµ¬ë¶„ ì œì‹œ
        """,
        "recommendations": """
        - 3ë‹¨ê³„ ì‹¤í–‰ ë¡œë“œë§µ ì œì‹œ(Phase 1/2/3, ê° ë‹¨ê³„ë³„ ê¸°ê°„ ëª…ì‹œ)
        - ê° ë‹¨ê³„ë³„ ì˜ˆìƒ íˆ¬ì ê·œëª¨($50K-$2M+ ë“± êµ¬ì²´ì  ë²”ìœ„)
        - í•µì‹¬ íŒŒíŠ¸ë„ˆì‹­ ì „ëµ(êµ¬ì²´ì ì¸ ë²¤ë”ëª…ì´ë‚˜ SI ì—…ì²´ëª… ì œì‹œ)
        - ì¸ì¬ í™•ë³´ ê³„íš(ì§ë¬´, ì¸ì›, ì˜ˆìƒ ì—°ë´‰ ë²”ìœ„ í¬í•¨)
        - ê° ë‹¨ê³„ë³„ ê¸°ëŒ€ íš¨ê³¼ì™€ ROI ëª©í‘œ ì œì‹œ
        - ì„±ê³µ ì¸¡ì • ì§€í‘œ(KPI) ì œì•ˆ
        """,
        "conclusion": """
        - ë³´ê³ ì„œì˜ í•µì‹¬ ë©”ì‹œì§€ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
        - Physical AIì˜ ì „ëµì  ì¤‘ìš”ì„± ê°•ì¡°
        - ì¡°ì§ì´ ì·¨í•´ì•¼ í•  ì¦‰ê°ì  í–‰ë™ 1-2ê°€ì§€ ì œì‹œ
        - ë¯¸ë˜ ì „ë§ì— ëŒ€í•œ ê°„ê²°í•œ ê²¬í•´ ì œì‹œ
        """
    }

    report_sections = {}

    for section_key, section_title in sections.items():
        guidelines = section_guidelines.get(section_key, "")

        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""ë‹¹ì‹ ì€ AI ë¶„ì•¼ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
'{section_title}' ì„¹ì…˜ì„ ê³µì‹ì ì´ê³  ì •í˜•í™”ëœ ì‚°ì—… íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ ì–‘ì‹ì— ë§ì¶° ì‘ì„±í•˜ì„¸ìš”.

**í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:**
- ì„¹ì…˜ ì œëª©ì„ í¬í•¨í•˜ì§€ ë§ê³  ë³¸ë¡ ë§Œ ì‘ì„±í•˜ì„¸ìš”
- synthesis_nodeì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, í†µê³„, ê¸ˆì•¡ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- ì‹¤ì œ ê¸°ì—…ëª…, ì œí’ˆëª…, í”„ë¡œì íŠ¸ëª…ì„ ì ê·¹ ì–¸ê¸‰í•˜ì„¸ìš”
- ì—…ê³„ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”
- ì¶œì²˜ë‚˜ ì‹œê¸°ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”
- ì •ì„±ì  ì„œìˆ ë³´ë‹¤ëŠ” ì •ëŸ‰ì  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

**ì„¹ì…˜ë³„ ì‘ì„± ê°€ì´ë“œë¼ì¸:**
{guidelines}

**ì‘ì„± ìŠ¤íƒ€ì¼:**
- ë¬¸ì¥ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ (1ë¬¸ì¥ 2ì¤„ ì´ë‚´)
- ê° ì£¼ì¥ì— ëŒ€í•œ ê·¼ê±°(ìˆ˜ì¹˜, ì‚¬ë¡€, ì¶œì²˜)ë¥¼ ë°˜ë“œì‹œ ì œì‹œ
- ì¶”ìƒì  í‘œí˜„(ë§ì€, ë¹ ë¥¸, ì¤‘ìš”í•œ ë“±) ì§€ì–‘í•˜ê³  êµ¬ì²´ì  í‘œí˜„ ì‚¬ìš©

ì´ ì„¹ì…˜ì€ ì „ì²´ ë³´ê³ ì„œì˜ ì¼ë¶€ì´ë¯€ë¡œ, ì œê³µëœ ë¶„ì„ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ í™œìš©í•˜ì—¬ ì „ë¬¸ì„± ìˆê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""),
            ("user", """ë¶„ì„ ë°ì´í„°:
{data}

ìœ„ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{section_title}' ì„¹ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”.
ê°€ëŠ¥í•œ ëª¨ë“  êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ì—…ëª…, ì œí’ˆëª…, ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì„¸ìš”.""")
        ])

        response = llm.invoke(
            prompt.format_messages(
                data=str(state["synthesized_data"]),
                section_title=section_title,
                guidelines=guidelines
            )
        )
        report_sections[section_key] = response.content

    state["report_sections"] = report_sections
    state["messages"].append("âœ… ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ (ì •í˜•í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì–‘ì‹ ì ìš©)")
    return state

def extract_sources_from_data(state: AgentState) -> str:
    """ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¶œì²˜ URL ì¶”ì¶œ"""
    sources = []
    seen_urls = set()
    
    for category in ["market_data", "tech_data", "industry_data", "company_data", "challenge_data"]:
        data_list = state.get(category, [])
        for item in data_list:
            results = item.get("results", [])
            for result in results[:2]:  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ
                url = result.get("url", "")
                title = result.get("title", "")
                if url and url not in seen_urls:
                    sources.append(f"- [{title}]({url[:10]})")
                    seen_urls.add(url)
    
    return "\n".join(sources[:40]) if sources else "- Tavily AI Search APIë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ë‹¤ì–‘í•œ ì˜¨ë¼ì¸ ì†ŒìŠ¤"

def structure_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ êµ¬ì¡°í™” ë° í¬ë§·íŒ… (ê²°ë¡ , ì¶œì²˜, í‰ê°€ê¸°ì¤€ í¬í•¨)"""
    sections = state["report_sections"]
    
    # ì¶œì²˜ ìë™ ì¶”ì¶œ
    sources = extract_sources_from_data(state)
    
    # Markdown í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ êµ¬ì¡°í™”
    final_report = f"""

# 2026-2030 AI ì‚°ì—… íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ

## í•µì‹¬ ìš”ì•½

{sections.get("executive_summary", "")}

---

## ì‹œì¥ ì „ë§

{sections.get("market_overview", "")}

---

## ê¸°ìˆ  íŠ¸ë Œë“œ

{sections.get("technology_trends", "")}

---

## ì‚°ì—…ë³„ ì‘ìš©

{sections.get("industry_applications", "")}

---

## ì£¼ìš” ê¸°ì—…

{sections.get("key_players", "")}

---

## ë„ì „ ê³¼ì œ

{sections.get("challenges", "")}

---

## í–¥í›„ 5ë…„ ì „ë§

{sections.get("forecast", "")}

---

## ì „ëµì  ê¶Œê³ ì‚¬í•­

{sections.get("recommendations", "")}

---

## ê²°ë¡ 

{sections.get("conclusion", "")}

---

## ì°¸ê³  ìë£Œ ë° ì¶œì²˜

ë³¸ ë³´ê³ ì„œëŠ” ë‹¤ìŒì˜ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤:

{sources}

**ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•:**
- Tavily AI Advanced Search APIë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰
- ì—…ê³„ ë³´ê³ ì„œ, ë‰´ìŠ¤ ê¸°ì‚¬, ê¸°ì—… ë°œí‘œ ìë£Œ ë“± ë‹¤ì–‘í•œ ê³µê°œ ì†ŒìŠ¤ ë¶„ì„
- AI ê¸°ë°˜ ì •ë³´ ì¢…í•© ë° íŠ¸ë Œë“œ ë¶„ì„

---

## Appendix: ë³´ê³ ì„œ í’ˆì§ˆ í‰ê°€ ê¸°ì¤€

ë³¸ ë³´ê³ ì„œëŠ” ë‹¤ìŒ 5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤:

### A. ë‚´ìš© ì™„ì„±ë„ (20ì  ë§Œì )
- í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ ì œì‹œ
- ê° ì„¹ì…˜ì˜ ê¹Šì´ì™€ í¬ê´„ì„±
- ë…¼ë¦¬ì  ì „ê°œì™€ ì¼ê´€ì„±

### B. ë°ì´í„° ì •í™•ì„± (20ì  ë§Œì )
- ìˆ˜ì¹˜ì™€ í†µê³„ì˜ ì •í™•ì„± ë° ìµœì‹ ì„±
- ì¶œì²˜ì˜ ì‹ ë¢°ë„ì™€ ëª…í™•ì„±
- íŒ©íŠ¸ ì²´í¬ ìˆ˜ì¤€

### C. êµ¬ì¡° ë…¼ë¦¬ì„± (20ì  ë§Œì )
- ì„¹ì…˜ ê°„ ì—°ê²°ì„±ê³¼ íë¦„
- ëª©ì°¨ êµ¬ì„±ì˜ ì²´ê³„ì„±
- ìš”ì•½-ë³¸ë¬¸-ê²°ë¡ ì˜ ì •í•©ì„±

### D. ì‹¤í–‰ ê°€ëŠ¥ì„± (20ì  ë§Œì )
- ê¶Œê³ ì‚¬í•­ì˜ êµ¬ì²´ì„±
- ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±
- ì‹¤í–‰ ë¡œë“œë§µ ì œì‹œ ì—¬ë¶€

### E. ì „ë¬¸ì„± (20ì  ë§Œì )
- ì—…ê³„ ìš©ì–´ì™€ ê°œë… í™œìš© ìˆ˜ì¤€
- í†µì°°ì˜ ê¹Šì´ì™€ ë…ì°½ì„±
- ì „ë¬¸ ë¦¬í¬íŠ¸ë¡œì„œì˜ ì™„ì„±ë„

"""
    
    state["final_report"] = final_report
    state["messages"].append("âœ… ë³´ê³ ì„œ êµ¬ì¡°í™” ì™„ë£Œ (ëª©ì°¨, ê²°ë¡ , ì¶œì²˜, í‰ê°€ê¸°ì¤€ í¬í•¨)")
    return state

def review_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ í’ˆì§ˆ ê²€í† """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    response = llm.invoke(
        REVIEW_FEW_SHOT_PROMPT.format_messages(
            baseline_report=REVIEW_BASELINE_REPORT,
            strong_report=REVIEW_STRONG_REPORT,
            report=state["final_report"][:4000]
        )
    )

    content = response.content.strip()
    score_match = re.search(r"ì ìˆ˜\s*[:=]\s*(\d+(?:\.\d+)?)", content)
    score = float(score_match.group(1)) if score_match else 7.0
    score = max(0.0, min(10.0, score))

    state.setdefault("search_context", {})
    state["search_context"]["review_feedback"] = content
    state["quality_score"] = score
    state["final_report"] += f"\n\n---\n\n## ë³´ê³ ì„œ í’ˆì§ˆ ê²€í†  ê²°ê³¼\n\n{content}"
    state["final_report"] += f"*ë³´ê³ ì„œ ìƒì„±ì¼: {__import__('datetime').datetime.now().strftime('%Yë…„ %mì›” %dì¼')}*\n*ìƒì„± ì‹œìŠ¤í…œ: Physical AI Trend Report Generator (Powered by LangGraph + Tavily AI)*"
    state["messages"].append(f"âœ… í’ˆì§ˆ ê²€í†  ì™„ë£Œ (ì ìˆ˜: {score:.1f}/10)")
    state["messages"].append("ğŸ“ ë¦¬ë·° ìš”ì•½ ì €ì¥")
    return state

def final_quality_check_node(state: AgentState) -> str:
    """ìµœì¢… í’ˆì§ˆ í™•ì¸"""
    if state["quality_score"] < 7.0 and state["iteration_count"] < 2:
        state["iteration_count"] += 1
        return "refine"
    return "format"

def refinement_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ ê°œì„ """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

    # review_feedback ê°€ì ¸ì˜¤ê¸°
    review_feedback = state.get("search_context", {}).get("review_feedback", "ë¦¬ë·° í”¼ë“œë°± ì—†ìŒ")

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë³´ê³ ì„œë¥¼ ê°œì„ í•˜ì„¸ìš”. ë‹¤ìŒì— ì§‘ì¤‘í•˜ì„¸ìš”:
        1. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë° ë°ì´í„° ì¶”ê°€
        2. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­ ê°•í™”
        3. ë…¼ë¦¬ì  íë¦„ ê°œì„ 

í’ˆì§ˆ ê²€í†  í”¼ë“œë°±:
{review_feedback}

ìœ„ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì§€ì ëœ ë¬¸ì œì ì„ ê°œì„ í•˜ì„¸ìš”."""),
        ("user", "í˜„ì¬ ë³´ê³ ì„œ:\n{report}\n\në¶„ì„ ë°ì´í„°:\n{data}")
    ])

    response = llm.invoke(
        prompt.format_messages(
            review_feedback=review_feedback,
            report=state["final_report"][:2000],
            data=str(state["synthesized_data"])[:1000]
        )
    )

    # ê°œì„ ëœ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    state["final_report"] = response.content
    state["messages"].append("âœ… ë³´ê³ ì„œ ê°œì„  ì™„ë£Œ")
    return state

def formatting_node(state: AgentState) -> AgentState:
    """ìµœì¢… í¬ë§·íŒ… ë° PDF ìƒì„±"""
    state["messages"].append("âœ… ìµœì¢… í¬ë§·íŒ… ì™„ë£Œ")
    
    # PDF ìƒì„±
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_filename = f"Ai_report_{timestamp}.pdf"
        markdown_to_pdf(state["final_report"], pdf_filename)
        state["messages"].append(f"ğŸ“„ PDF ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {pdf_filename}")
    except Exception as e:
        state["messages"].append(f"âš ï¸ PDF ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    return state

# ==================== ê·¸ë˜í”„ êµ¬ì„± ====================

def create_physical_ai_agent():
    """AI ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„±"""
    
    # StateGraph ì´ˆê¸°í™”
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("planning", planning_node)
    workflow.add_node("market_research", market_research_node)
    workflow.add_node("tech_research", tech_research_node)
    workflow.add_node("industry_research", industry_research_node)
    workflow.add_node("company_research", company_research_node)
    workflow.add_node("challenge_research", challenge_research_node)
    workflow.add_node("synthesis", synthesis_node)
    workflow.add_node("report_generation", report_generation_node)
    workflow.add_node("structure", structure_node)
    workflow.add_node("review", review_node)
    workflow.add_node("refinement", refinement_node)
    workflow.add_node("formatting", formatting_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("planning")
    
    # Planning -> Research ë³‘ë ¬ ì‹¤í–‰
    workflow.add_edge("planning", "market_research")
    workflow.add_edge("planning", "tech_research")
    workflow.add_edge("planning", "industry_research")
    workflow.add_edge("planning", "company_research")
    workflow.add_edge("planning", "challenge_research")
    
    # Research -> Synthesis
    workflow.add_edge("market_research", "synthesis")
    workflow.add_edge("tech_research", "synthesis")
    workflow.add_edge("industry_research", "synthesis")
    workflow.add_edge("company_research", "synthesis")
    workflow.add_edge("challenge_research", "synthesis")
    
    # Synthesis -> Quality Check (ì¡°ê±´ë¶€)
    workflow.add_conditional_edges(
        "synthesis",
        quality_check_node,
        {
            "research_more": "planning",
            "generate_report": "report_generation"
        }
    )
    
    # Report Generation -> Structure
    workflow.add_edge("report_generation", "structure")
    
    # Structure -> Review
    workflow.add_edge("structure", "review")
    
    # Review -> Final Check (ì¡°ê±´ë¶€)
    workflow.add_conditional_edges(
        "review",
        final_quality_check_node,
        {
            "refine": "refinement",
            "format": "formatting"
        }
    )
    
    # Refinement -> Structure (ê°œì„  í›„ ì¬ê²€í† )
    workflow.add_edge("refinement", "structure")
    
    # Formatting -> END
    workflow.add_edge("formatting", END)
    
    return workflow.compile()

# ==================== ì‹¤í–‰ ì˜ˆì œ ====================

def run_agent(user_query: str):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    
    # Tavily API í‚¤ í™•ì¸
    if not os.environ.get("TAVILY_API_KEY"):
        print("âš ï¸  ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("https://tavily.com ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
        return None
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "user_query": user_query,
        "research_plan": {},
        "market_data": [],
        "tech_data": [],
        "industry_data": [],
        "company_data": [],
        "challenge_data": [],
        "synthesized_data": {},
        "report_sections": {},
        "final_report": "",
        "quality_score": 0.0,
        "iteration_count": 0,
        "search_context": {},  # Tavily AI ë‹µë³€ ì €ì¥
        "messages": []
    }
    
    print("ğŸš€ AI íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
    print(f"ğŸ“ ìš”ì²­: {user_query}\n")
    
    # ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰
    agent = create_physical_ai_agent()
    result = agent.invoke(initial_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ì‹¤í–‰ ë¡œê·¸")
    print("="*60)
    for msg in result["messages"]:
        print(msg)
    
    print("\n" + "="*60)
    print("ğŸ“„ ìµœì¢… ë³´ê³ ì„œ")
    print("="*60)
    print(result["final_report"])
    
    print("\n" + "="*60)
    print("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ˆ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/10")
    print(f"ğŸ”„ ë°˜ë³µ íšŸìˆ˜: {result['iteration_count']}")
    print("="*60)
    
    return result

# ì‹¤í–‰
if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”
    # export OPENAI_API_KEY="your-openai-key"
    # export TAVILY_API_KEY="your-tavily-key"
    
    query = "í–¥í›„ 5ë…„ ì´ë‚´ ê¸°ì—…ì—ì„œ ê´€ì‹¬ìˆê²Œ ë´ì•¼í•  AI íŠ¸ë Œë“œ"
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         AI íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸              â•‘
â•‘              Powered by LangGraph + Tavily                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    result = run_agent(query)
    
    if result:
        # ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"Ai_report_{timestamp}.md"
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(result["final_report"])
        
        print(f"\nğŸ’¾ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")