"""
Physical AI íŠ¸ë Œë“œ ì˜ˆì¸¡ ë³´ê³ ì„œ ìƒì„± AI ì—ì´ì „íŠ¸
LangGraph êµ¬í˜„ with Tavily Search
"""

from typing import TypedDict, Annotated, List, Dict, Optional, Any
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
import operator
import os
import re
from datetime import datetime
from dotenv import load_dotenv

# PDF ìƒì„± ë¼ì´ë¸ŒëŸ¬ë¦¬
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

from pathlib import Path
import os

FONT_DIRS = [
    Path(os.environ.get("LOCALAPPDATA", "")) / "Microsoft" / "Windows" / "Fonts",
    Path(r"C:\Windows\Fonts"),
]

def find_font(filename):
    for base in FONT_DIRS:
        candidate = base / filename
        if candidate.exists():
            return str(candidate)
    raise FileNotFoundError(filename)


# ==================== PDF ì„¤ì • ====================
# í•œê¸€ í°íŠ¸ ë“±ë¡
try:
    pdfmetrics.registerFont(TTFont("NanumGothic", find_font("NanumGothic-Regular.ttf")))
    pdfmetrics.registerFont(TTFont("NanumGothicBold", find_font("NanumGothic-Bold.ttf")))
    KOREAN_FONT = 'NanumGothic'
    KOREAN_FONT_BOLD = 'NanumGothicBold'
except:
    KOREAN_FONT = 'Helvetica'
    KOREAN_FONT_BOLD = 'Helvetica-Bold'
    print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

load_dotenv(override=True)

# ì»¬ëŸ¬ íŒ”ë ˆíŠ¸
PRIMARY_COLOR = colors.HexColor('#1E3A8A')
SECONDARY_COLOR = colors.HexColor('#3B82F6')
ACCENT_COLOR = colors.HexColor('#06B6D4')
LIGHT_BG = colors.HexColor('#F0F9FF')


class NumberedCanvas(canvas.Canvas):
    """í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ìº”ë²„ìŠ¤"""
    def __init__(self, *args, **kwargs):
        canvas.Canvas.__init__(self, *args, **kwargs)
        self._saved_page_states = []
        
    def showPage(self):
        self._saved_page_states.append(dict(self.__dict__))
        self._startPage()
        
    def save(self):
        num_pages = len(self._saved_page_states)
        for state in self._saved_page_states:
            self.__dict__.update(state)
            self.draw_page_number(num_pages)
            canvas.Canvas.showPage(self)
        canvas.Canvas.save(self)
        
    def draw_page_number(self, page_count):
        """í˜ì´ì§€ ë²ˆí˜¸ ê·¸ë¦¬ê¸°"""
        page_num = self._pageNumber
        if page_num > 1:
            self.setFont(KOREAN_FONT, 9)
            self.setFillColor(colors.grey)
            self.drawRightString(A4[0] - 50, 25, f"Page {page_num - 1} of {page_count - 1}")
            self.drawString(50, 25, "Physical AI Trend Report")


def markdown_to_pdf(markdown_text: str, output_filename: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ë³€í™˜"""
    
    doc = SimpleDocTemplate(
        output_filename,
        pagesize=A4,
        rightMargin=50,
        leftMargin=50,
        topMargin=50,
        bottomMargin=50
    )
    
    # ìŠ¤íƒ€ì¼ ì •ì˜
    styles = getSampleStyleSheet()
    
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Title'],
        fontSize=28,
        textColor=PRIMARY_COLOR,
        alignment=TA_CENTER,
        spaceAfter=30,
        fontName=KOREAN_FONT_BOLD
    )
    
    h1_style = ParagraphStyle(
        'CustomH1',
        parent=styles['Heading1'],
        fontSize=20,
        textColor=PRIMARY_COLOR,
        spaceAfter=12,
        spaceBefore=20,
        fontName=KOREAN_FONT_BOLD,
        borderWidth=2,
        borderColor=ACCENT_COLOR,
        borderPadding=8,
        backColor=LIGHT_BG
    )
    
    h2_style = ParagraphStyle(
        'CustomH2',
        parent=styles['Heading2'],
        fontSize=16,
        textColor=SECONDARY_COLOR,
        spaceAfter=10,
        spaceBefore=15,
        fontName=KOREAN_FONT_BOLD
    )
    
    h3_style = ParagraphStyle(
        'CustomH3',
        parent=styles['Heading3'],
        fontSize=14,
        textColor=SECONDARY_COLOR,
        spaceAfter=8,
        spaceBefore=12,
        fontName=KOREAN_FONT_BOLD
    )
    
    body_style = ParagraphStyle(
        'CustomBody',
        parent=styles['Normal'],
        fontSize=11,
        alignment=TA_JUSTIFY,
        spaceAfter=10,
        leading=16,
        fontName=KOREAN_FONT
    )
    
    story = []
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
    lines = markdown_text.split('\n')
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        # ë¹ˆ ì¤„
        if not line:
            i += 1
            continue
        
        # ì œëª©
        if line.startswith('# '):
            title_text = line[2:].strip()
            if i < 3:  # ì²˜ìŒ ë‚˜ì˜¤ëŠ” ì œëª©ì€ ë©”ì¸ íƒ€ì´í‹€
                story.append(Spacer(1, 0.5*inch))
                story.append(Paragraph(title_text, title_style))
                story.append(Spacer(1, 0.3*inch))
            else:
                story.append(Paragraph(title_text, h1_style))
            i += 1
            
        elif line.startswith('## '):
            story.append(Paragraph(line[3:].strip(), h2_style))
            i += 1
            
        elif line.startswith('### '):
            story.append(Paragraph(line[4:].strip(), h3_style))
            i += 1
        
        # ìˆ˜í‰ì„ 
        elif line.startswith('---'):
            story.append(Spacer(1, 0.2*inch))
            i += 1
        
        # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸
        elif line.startswith('- ') or line.startswith('* '):
            bullet_text = line[2:].strip()
            bullet_text = f"â€¢ {bullet_text}"
            story.append(Paragraph(bullet_text, body_style))
            i += 1
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸
        else:
            # ë³¼ë“œ ì²˜ë¦¬ (**text**)
            line = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', line)
            # ì´íƒ¤ë¦­ ì²˜ë¦¬ (*text*)
            line = re.sub(r'\*(.*?)\*', r'<i>\1</i>', line)
            
            story.append(Paragraph(line, body_style))
            i += 1
    
    # ë°œí–‰ì¼ ì¶”ê°€
    story.append(Spacer(1, 0.5*inch))
    date_style = ParagraphStyle(
        'DateStyle',
        parent=styles['Normal'],
        fontSize=10,
        textColor=colors.grey,
        alignment=TA_CENTER,
        fontName=KOREAN_FONT
    )
    story.append(Paragraph(f"<i>ë³´ê³ ì„œ ìƒì„±ì¼: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}</i>", date_style))
    
    # PDF ìƒì„±
    doc.build(story, canvasmaker=NumberedCanvas)
    
    return output_filename


# ==================== ë¦¬ì„œì¹˜ ê³„íš êµ¬ì¡° ====================
class ResearchArea(BaseModel):
    """LLMì´ ë°˜í™˜í•˜ëŠ” ê° ì—°êµ¬ ì˜ì—­ êµ¬ì¡°"""
    focus_question: str = Field(..., description="í•´ë‹¹ ì˜ì—­ì—ì„œ ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸")
    search_keywords: List[str] = Field(..., description="í•´ë‹¹ ì˜ì—­ì„ íƒìƒ‰í•  ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ (ìµœì†Œ 5ê°œ)")
    expected_insights: str = Field(..., description="ì¡°ì‚¬ë¥¼ í†µí•´ ë„ì¶œí•˜ê³ ì í•˜ëŠ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸")


class ResearchPlan(BaseModel):
    """LLMì´ ìƒì„±í•˜ëŠ” ì „ì²´ ë¦¬ì„œì¹˜ ê³„íš"""
    plan_overview: str = Field(..., description="ì „ì²´ ë¦¬ì„œì¹˜ ë°©í–¥ê³¼ ëª©í‘œ ìš”ì•½")
    market: ResearchArea
    tech: ResearchArea
    industry: ResearchArea
    company: ResearchArea
    challenge: ResearchArea


DEFAULT_RESEARCH_PLAN: Dict[str, List[str]] = {
    "market": [
        "Physical AI market size 2025-2030",
        "humanoid robotics market forecast",
        "AI robotics investment trends",
        "Physical AI market growth rate",
        "robotics market segmentation"
    ],
    "tech": [
        "Vision-Language-Action models 2025",
        "Physical AI foundation models",
        "robotics AI breakthrough 2025",
        "NVIDIA Isaac GR00T",
        "world foundation models robotics"
    ],
    "industry": [
        "Physical AI manufacturing use cases",
        "robotics logistics warehouse 2025",
        "AI robotics healthcare applications",
        "autonomous vehicles Physical AI",
        "retail robotics deployment"
    ],
    "company": [
        "Tesla Optimus humanoid robot",
        "Figure AI BMW partnership",
        "Agility Robotics Digit",
        "Boston Dynamics Atlas electric",
        "NVIDIA Cosmos robotics"
    ],
    "challenge": [
        "Physical AI adoption barriers",
        "robotics battery life challenges",
        "AI robot safety concerns",
        "workforce robotics impact",
        "Physical AI regulation 2025"
    ]
}

planning_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ Physical AI ë¶„ì•¼ì˜ ì‹œë‹ˆì–´ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
í–¥í›„ 5ë…„ì„ ëŒ€ìƒìœ¼ë¡œ ì‹œì¥, ê¸°ìˆ , ì‚°ì—…, ê¸°ì—…, ë„ì „ê³¼ì œ ë‹¤ì„¯ ì˜ì—­ì— ëŒ€í•œ ë¦¬ì„œì¹˜ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
ì‘ë‹µì€ JSON í˜•ì‹ì´ë©°, ê° ì˜ì—­ì— ëŒ€í•´ focus_question, search_keywords, expected_insightsë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
search_keywordsëŠ” ì˜ì–´ë¡œ ì œê³µí•˜ë©° í•­ìƒ 5ê°œ ì´ìƒì˜ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë˜ëŠ” ë¬¸êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""),
    ("user", """ìš”ì²­: {query}

ë¶„ì„ ê¸°ê°„: í–¥í›„ 5ë…„

ê° ì˜ì—­ë³„ ì¡°ì‚¬ ëª©í‘œë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ê³ , ê²€ìƒ‰ í‚¤ì›Œë“œ 5ê°œì™€ ê¸°ëŒ€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œì‹œí•˜ì„¸ìš”.""")
])

planning_chain = planning_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0).with_structured_output(ResearchPlan)

QUALITY_CRITERIA = {
    "min_items": 3,
    "min_hit_ratio": 0.6,
    "min_avg_score": 0.25,
    "min_answer_ratio": 0.4,
    "min_content_ratio": 0.5,
    "min_answer_chars": 80,
    "min_content_chars": 120,
}

REVIEW_BASELINE_REPORT = """# Physical AI ë³´ê³ ì„œ ìš”ì•½ (ì´ˆì•ˆ)
- ì‹œì¥ ì „ë§ì´ ì •ì„±ì  ì„œìˆ ì— ë¨¸ë¬¼ëŸ¬ ìˆìœ¼ë©° ìˆ˜ì¹˜ ê·¼ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.
- ê¸°ìˆ  ì„¹ì…˜ì€ íŠ¸ë Œë“œ í‚¤ì›Œë“œë§Œ ë‚˜ì—´í•˜ê³  ì‹¤ì œ êµ¬í˜„ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.
- ì‚°ì—…ë³„ ë¶„ì„ì´ ë‹¨ìˆœíˆ ì—…ì¢…ì„ ì—´ê±°í•˜ëŠ” ìˆ˜ì¤€ì´ë¼ ì‹¤í–‰ í†µì°°ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.
"""

REVIEW_STRONG_REPORT = """# Physical AI ë³´ê³ ì„œ ìš”ì•½ (ìš°ìˆ˜)
- ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„°ë¥¼ CAGRê³¼ íˆ¬ì ê¸ˆì•¡ìœ¼ë¡œ ì œì‹œí•˜ê³  ì§€ì—­ë³„ ì°¨ë³„í™” í¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- ê¸°ìˆ  ì„¹ì…˜ì€ GAI, ë¡œë³´í‹±ìŠ¤ ì†Œí”„íŠ¸ì›¨ì–´ ìŠ¤íƒ ë“± í•µì‹¬ ê¸°ìˆ ì„ ìµœì‹  ì‚¬ë¡€ì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì‚°ì—… ì ìš©ê³¼ ë„ì „ê³¼ì œ, ê¶Œê³ ì•ˆì´ ì„œë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ì „ëµì  ì‹¤í–‰ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
"""

REVIEW_FEW_SHOT_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ Physical AI ì „ë¬¸ ê°ìˆ˜ìì…ë‹ˆë‹¤. ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ 0-10ì  ì‚¬ì´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³ , ê° í•­ëª©ë³„ ì½”ë©˜íŠ¸ë¥¼ ë‚¨ê¸°ì„¸ìš”.
- ë‚´ìš© ì™„ì„±ë„: í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì—¬ë¶€ì™€ ê¹Šì´
- ë°ì´í„° ì •í™•ì„±: ìˆ˜ì¹˜, ì¶œì²˜, ì‚¬ì‹¤ ê²€ì¦ ìˆ˜ì¤€
- êµ¬ì¡° ë…¼ë¦¬ì„±: ì„¹ì…˜ ê°„ ì—°ê²°ì„±ê³¼ ì´ì•¼ê¸° íë¦„
- ì‹¤í–‰ ê°€ëŠ¥ì„±: ê¶Œê³ ì‚¬í•­ì˜ êµ¬ì²´ì„± ë° ì‹¤í˜„ì„±
- ì „ë¬¸ì„±: ì—…ê³„ ìš©ì–´ í™œìš©ê³¼ í†µì°°ì˜ ìˆ˜ì¤€

í•­ìƒ ì•„ë˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”:
ì ìˆ˜: x.x/10
ê°•ì :
- ...
ê°œì„  í•„ìš”:
- ...
ì„¸ë¶€ í‰ê°€:
- ë‚´ìš© ì™„ì„±ë„: ...
- ë°ì´í„° ì •í™•ì„±: ...
- êµ¬ì¡° ë…¼ë¦¬ì„±: ...
- ì‹¤í–‰ ê°€ëŠ¥ì„±: ...
- ì „ë¬¸ì„±: ..."""),
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ:
{baseline_report}"""),
    ("assistant", """ì ìˆ˜: 5.8/10
ê°•ì :
- Physical AI ì‹œì¥ì´ ì„±ì¥ ì¤‘ì´ë¼ëŠ” ë°©í–¥ì„±ì€ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
ê°œì„  í•„ìš”:
- ìˆ˜ì¹˜ ê·¼ê±°ì™€ ìµœê·¼ ì‚¬ë¡€ê°€ ì—†ì–´ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.
- ê¶Œê³ ì•ˆì´ ì¶”ìƒì ì´ë©° ì‹¤í–‰ ë‹¨ê³„ë¥¼ ì œì‹œí•˜ì§€ ëª»í•©ë‹ˆë‹¤.
ì„¸ë¶€ í‰ê°€:
- ë‚´ìš© ì™„ì„±ë„: í•µì‹¬ ì§ˆë¬¸ì„ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë‹¤ë£¹ë‹ˆë‹¤.
- ë°ì´í„° ì •í™•ì„±: ì •ì„±ì  ì£¼ì¥ë§Œ ìˆê³  ê·¼ê±° ìˆ˜ì¹˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.
- êµ¬ì¡° ë…¼ë¦¬ì„±: ì„¹ì…˜ ê°„ ì—°ê²°ì´ ì•½í•˜ê³  ì¤‘ë³µì´ ìˆìŠµë‹ˆë‹¤.
- ì‹¤í–‰ ê°€ëŠ¥ì„±: ê¶Œê³ ì•ˆì´ "í˜‘ë ¥ ê°•í™”" ìˆ˜ì¤€ì— ë¨¸ë­…ë‹ˆë‹¤.
- ì „ë¬¸ì„±: ì—…ê³„ ì§€ì‹ê³¼ ìš©ì–´ í™œìš©ì´ ì œí•œì ì…ë‹ˆë‹¤."""),
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ:
{strong_report}"""),
    ("assistant", """ì ìˆ˜: 8.9/10
ê°•ì :
- ì‹œì¥Â·ê¸°ìˆ Â·ì‚°ì—… ë°ì´í„°ë¥¼ ìµœì‹  ìˆ˜ì¹˜ì™€ ì¸ìš©ìœ¼ë¡œ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.
- ê¶Œê³ ì•ˆì´ íˆ¬ì ìš°ì„ ìˆœìœ„ì™€ ì‹¤í–‰ ë‹¨ê³„ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
ê°œì„  í•„ìš”:
- ìœ„í—˜ ê´€ë¦¬ ì „ëµì— ëŒ€í•œ ëŒ€ì•ˆ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì¡°ê¸ˆ ë” ìˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.
ì„¸ë¶€ í‰ê°€:
- ë‚´ìš© ì™„ì„±ë„: ìš”êµ¬ëœ ì§ˆë¬¸ì„ ëª¨ë‘ ê¹Šì´ ìˆê²Œ ë‹µí–ˆìŠµë‹ˆë‹¤.
- ë°ì´í„° ì •í™•ì„±: ë‹¤ìˆ˜ì˜ ìµœì‹  í†µê³„ì™€ ë³´ê³ ì„œë¥¼ ì •í™•íˆ ì¸ìš©í•©ë‹ˆë‹¤.
- êµ¬ì¡° ë…¼ë¦¬ì„±: ì„¹ì…˜ ì „ê°œê°€ ë§¤ë„ëŸ½ê³  ìš”ì•½-ë³¸ë¬¸-ê¶Œê³ ê°€ ì •ë ¬ë©ë‹ˆë‹¤.
- ì‹¤í–‰ ê°€ëŠ¥ì„±: ì¬ë¬´Â·ê¸°ìˆ Â·íŒŒíŠ¸ë„ˆì‹­ ê´€ì ì—ì„œ ì‹¤ë¬´ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì „ë¬¸ì„±: ì—…ê³„ ë™í–¥ê³¼ ìš©ì–´ í™œìš©ì´ ì „ë¬¸ ë¦¬í¬íŠ¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤."""),
    ("human", """í‰ê°€ ëŒ€ìƒ ë³´ê³ ì„œ:
{report}

ìœ„ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ì„¸ìš”."""),
])

# ==================== Tavily ê²€ìƒ‰ ì„¤ì • ====================
def get_tavily_search(max_results: int = 5):
    """Tavily ê²€ìƒ‰ ë„êµ¬ ìƒì„±"""
    return TavilySearchResults(
        max_results=max_results,
        search_depth="advanced",  # "basic" or "advanced"
        include_answer=True,  # AI ìƒì„± ë‹µë³€ í¬í•¨
        include_raw_content=False,  # only summary text to keep logs small
        include_images=False
    )

# ==================== ìƒíƒœ ë³‘í•© ìœ í‹¸ë¦¬í‹° ====================
def preserve_user_query(existing: Optional[str], new: str) -> str:
    """Keep the first user query value when multiple nodes emit it."""
    return existing if existing else new

def replace_research_plan(existing: Optional[Dict[str, List[str]]], new: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """Prefer the newest non-empty research plan."""
    if new:
        return new
    return existing or {}

def replace_result_list(existing: Optional[List[Dict]], new: List[Dict]) -> List[Dict]:
    """Prefer the latest non-empty research results list."""
    if new:
        return new
    return existing or []

def merge_search_context(existing: Optional[Dict[str, str]], new: Dict[str, str]) -> Dict[str, str]:
    """Merge search context dictionaries across parallel nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged

def merge_report_sections(existing: Optional[Dict[str, str]], new: Dict[str, str]) -> Dict[str, str]:
    """Merge report section text updates across nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged


def merge_synthesized_data(existing: Optional[Dict[str, Any]], new: Dict[str, Any]) -> Dict[str, Any]:
    """Merge synthesized insights coming from multiple nodes."""
    merged = dict(existing) if existing else {}
    merged.update(new)
    return merged

def replace_final_report(existing: Optional[str], new: str) -> str:
    """Prefer the latest final report content."""
    return new if new else (existing or "")

def replace_quality_score(existing: Optional[float], new: float) -> float:
    """Prefer the latest quality score provided."""
    return new if new is not None else (existing if existing is not None else 0.0)

def replace_iteration_count(existing: Optional[int], new: int) -> int:
    """Keep the highest iteration count emitted."""
    existing_val = existing if existing is not None else 0
    new_val = new if new is not None else 0
    return max(existing_val, new_val)

# ==================== State ì •ì˜ ====================
class AgentState(TypedDict):
    """ì—ì´ì „íŠ¸ ìƒíƒœ ê´€ë¦¬"""
    user_query: Annotated[str, preserve_user_query]
    research_plan: Annotated[Dict[str, List[str]], replace_research_plan]
    market_data: Annotated[List[Dict], replace_result_list]
    tech_data: Annotated[List[Dict], replace_result_list]
    industry_data: Annotated[List[Dict], replace_result_list]
    company_data: Annotated[List[Dict], replace_result_list]
    challenge_data: Annotated[List[Dict], replace_result_list]
    synthesized_data: Annotated[Dict[str, Any], merge_synthesized_data]
    report_sections: Annotated[Dict[str, str], merge_report_sections]
    final_report: Annotated[str, replace_final_report]
    quality_score: Annotated[float, replace_quality_score]
    iteration_count: Annotated[int, replace_iteration_count]
    search_context: Annotated[Dict[str, str], merge_search_context]  # Tavily ë‹µë³€ ì €ì¥
    messages: Annotated[List[str], operator.add]


def execute_tavily_query(tavily_search, query: str) -> Dict[str, Any]:
    """Run Tavily with fallbacks to avoid empty responses."""
    # âœ… ë‹¨ìˆœí™”ëœ ì‹œë„ ëª©ë¡ (ë¬¸ìì—´ë§Œ)
    attempts = [
        query,  # ì›ë³¸ ì¿¼ë¦¬
        f"{query} 2024 2025",  # ì—°ë„ ì¶”ê°€
        f"{query} analysis",  # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
    ]
    last_error = ""

    for idx, query_str in enumerate(attempts, 1):
        try:
            print(f"ğŸ” ì‹œë„ {idx}: {query_str[:50]}...")
            result = tavily_search.invoke(query_str)  # âœ… ë¬¸ìì—´ ì§ì ‘ ì „ë‹¬
            print(f"âœ… ì„±ê³µ! ê²°ê³¼ íƒ€ì…: {type(result)}")
            
            # ê²°ê³¼ ì²˜ë¦¬
            if isinstance(result, dict):
                if result.get("results"):
                    result.setdefault("answer", "")
                    result.setdefault("error", "")
                    return result
                last_error = result.get("error", "no results")
                print(f"âš ï¸ ë¹ˆ ê²°ê³¼: {last_error}")
                continue

            if isinstance(result, list):
                if result:
                    return {"results": result, "answer": "", "error": ""}
                print(f"âš ï¸ ë¹ˆ ë¦¬ìŠ¤íŠ¸")
                continue
                
        except Exception as exc:
            last_error = str(exc)
            print(f"âŒ ì—ëŸ¬: {exc}")
            
            # 432 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
            if "432" in str(exc):
                print("\nâš ï¸ Tavily API 432 ì—ëŸ¬ ë°œìƒ!")
                print("ê°€ëŠ¥í•œ ì›ì¸:")
                print("1. API í‚¤ê°€ ë¬´íš¨í•˜ê±°ë‚˜ ë§Œë£Œë¨")
                print("2. ë¬´ë£Œ í”Œëœ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ (https://app.tavily.com/home ì—ì„œ í™•ì¸)")
                print("3. API í‚¤ ê¶Œí•œ ë¬¸ì œ")
                print("\ní•´ê²° ë°©ë²•:")
                print("- ìƒˆ API í‚¤ ë°œê¸‰: https://app.tavily.com/")
                print("- í™˜ê²½ë³€ìˆ˜ ì¬ì„¤ì •: export TAVILY_API_KEY='your-new-key'")
                break  # 432 ì—ëŸ¬ëŠ” ì¬ì‹œë„ ë¶ˆí•„ìš”
            
            continue

    return {"results": [], "answer": "", "error": last_error or "empty Tavily response"}

def strengthen_keyword(category: str, keyword: str, state: AgentState) -> str:
    """Adjust Tavily query with quality feedback to target weak metrics."""
    feedback = state.get("search_context", {}).get("quality_feedback", {})
    issues = feedback.get(category, [])
    if not issues:
        return keyword

    modifiers: List[str] = []
    if "items" in issues or "coverage" in issues:
        modifiers.append("comprehensive data reliable sources 2024")
    if "answers" in issues:
        modifiers.append("key insights summary")
    if "content" in issues:
        modifiers.append("detailed report in depth analysis")
    if "score" in issues:
        modifiers.append("official statistics verified figures")

    if modifiers:
        boost = " ".join(modifiers)
        if boost not in keyword:
            return f"{keyword} {boost}"
    return keyword
# ==================== ë…¸ë“œ í•¨ìˆ˜ë“¤ ====================

def planning_node(state: AgentState) -> AgentState:
    """ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½"""
    state.setdefault("search_context", {})
    try:
        plan = planning_chain.invoke({
            "query": state["user_query"]
        })
        state["research_plan"] = {
            "market": list(plan.market.search_keywords),
            "tech": list(plan.tech.search_keywords),
            "industry": list(plan.industry.search_keywords),
            "company": list(plan.company.search_keywords),
            "challenge": list(plan.challenge.search_keywords)
        }
        state["search_context"]["plan_overview"] = plan.plan_overview
        for area in ("market", "tech", "industry", "company", "challenge"):
            area_plan = getattr(plan, area)
            state["search_context"][f"{area}_focus"] = area_plan.focus_question
        state["messages"].append("âœ… LLM ê¸°ë°˜ ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ")
        print("âœ… ë¦¬ì„œì¹˜ ê³„íš: ìˆ˜ë¦½ì™„ë£Œ")
        overview = plan.plan_overview.strip()
        if overview:
            state["messages"].append(f"ğŸ§­ ê³„íš ìš”ì•½: {overview}")
    except Exception as exc:
        print(f"ë¦¬ì„œì¹˜ ê³„íš ìƒì„± ì˜¤ë¥˜: {exc}")
        state["research_plan"] = {category: list(keywords) for category, keywords in DEFAULT_RESEARCH_PLAN.items()}
        state["search_context"]["plan_overview"] = "ì‚¬ì „ ì •ì˜ëœ ê¸°ë³¸ ë¦¬ì„œì¹˜ ê³„íš ì‚¬ìš©"
        state["messages"].append("âš ï¸ ê¸°ë³¸ ë¦¬ì„œì¹˜ ê³„íšìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤")
    return state

def market_research_node(state: AgentState) -> AgentState:
    """ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    market_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["market"]:
        try:
            # Tavily ê²€ìƒ‰ ì‹¤í–‰
            query = strengthen_keyword("market", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            # ê²°ê³¼ ì²˜ë¦¬ (TavilyëŠ” êµ¬ì¡°í™”ëœ Dict ë°˜í™˜)
            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],  # 500ì ì œí•œ
                    "score": result.get("score", 0.0)  # ê´€ë ¨ì„± ì ìˆ˜
                })
            
            market_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value  # Tavily AI ë‹µë³€
            })
            
            # AI ë‹µë³€ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            market_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["market_data"] = market_data
    
    # search_context ì´ˆê¸°í™” (ì²˜ìŒ ì‹¤í–‰ì‹œ)
    if "search_context" not in state:
        state["search_context"] = {}
    state["search_context"]["market"] = "\n".join(search_contexts)
    
    state["messages"].append(f"âœ… ì‹œì¥ ë°ì´í„° {len(market_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def tech_research_node(state: AgentState) -> AgentState:
    """ê¸°ìˆ  ë™í–¥ ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    tech_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["tech"]:
        try:
            query = strengthen_keyword("tech", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            tech_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            tech_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["tech_data"] = tech_data
    state["search_context"]["tech"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ê¸°ìˆ  ë°ì´í„° {len(tech_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def industry_research_node(state: AgentState) -> AgentState:
    """ì‚°ì—…ë³„ ì‚¬ë¡€ ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    industry_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["industry"]:
        try:
            query = strengthen_keyword("industry", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            industry_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            industry_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["industry_data"] = industry_data
    state["search_context"]["industry"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ì‚°ì—… ë°ì´í„° {len(industry_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def company_research_node(state: AgentState) -> AgentState:
    """ê¸°ì—… ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    company_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["company"]:
        try:
            query = strengthen_keyword("company", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            company_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            company_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["company_data"] = company_data
    state["search_context"]["company"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ê¸°ì—… ë°ì´í„° {len(company_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def challenge_research_node(state: AgentState) -> AgentState:
    """ë„ì „ê³¼ì œ ë°ì´í„° ìˆ˜ì§‘ with Tavily"""
    tavily_search = get_tavily_search(max_results=5)
    challenge_data = []
    search_contexts = []
    
    for keyword in state["research_plan"]["challenge"]:
        try:
            query = strengthen_keyword("challenge", keyword, state)
            raw_results = execute_tavily_query(tavily_search, query)
            answer_value = raw_results.get("answer", "") or ""
            results_list = raw_results.get("results", []) or []
            error_message = raw_results.get("error", "")
            if error_message and not results_list:
                search_contexts.append(f"[{keyword}] ERROR: {error_message}")
            if isinstance(results_list, dict):
                results_list = [results_list]
            elif not isinstance(results_list, list):
                results_list = []
            

            processed_results = []
            for result in results_list[:2]:
                if not isinstance(result, dict):
                    continue
                processed_results.append({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:250],
                    "score": result.get("score", 0.0)
                })
            
            challenge_data.append({
                "keyword": keyword,
                "query": query,
                "results": processed_results,
                "error": raw_results.get("error", ""),
                "answer": answer_value
            })
            
            if answer_value:
                search_contexts.append(f"[{keyword}]: {answer_value}")
                
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {keyword} - {e}")
            challenge_data.append({
                "keyword": keyword,
                "results": [],
                "answer": ""
            })
    
    state["challenge_data"] = challenge_data
    state["search_context"]["challenge"] = "\n".join(search_contexts)
    state["messages"].append(f"âœ… ë„ì „ê³¼ì œ ë°ì´í„° {len(challenge_data)}ê±´ ìˆ˜ì§‘ (Tavily Advanced)")
    return state

def synthesis_node(state: AgentState) -> AgentState:
    """ìˆ˜ì§‘ëœ ë°ì´í„° í†µí•© ë° ë¶„ì„ (Tavily ë‹µë³€ í™œìš©)"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    # Tavily AI ë‹µë³€ì„ ìš°ì„  í™œìš©
    all_data = {
        "market": {
            "tavily_answers": state["search_context"].get("market", ""),
            "detailed_data": state["market_data"]
        },
        "tech": {
            "tavily_answers": state["search_context"].get("tech", ""),
            "detailed_data": state["tech_data"]
        },
        "industry": {
            "tavily_answers": state["search_context"].get("industry", ""),
            "detailed_data": state["industry_data"]
        },
        "company": {
            "tavily_answers": state["search_context"].get("company", ""),
            "detailed_data": state["company_data"]
        },
        "challenge": {
            "tavily_answers": state["search_context"].get("challenge", ""),
            "detailed_data": state["challenge_data"]
        }
    }
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ Physical AI ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        Tavily AIê°€ ì œê³µí•œ ë‹µë³€ê³¼ ìƒì„¸ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
        ê° ì˜ì—­ë³„ë¡œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¸ë Œë“œ 3-5ê°€ì§€ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜, ìë£Œì˜ ì¶œì²˜ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”."""),
        ("user", """ì¹´í…Œê³ ë¦¬: {category}
        
Tavily AI ë‹µë³€:
{tavily_answers}

ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ ê²°ê³¼):
{detailed_data}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.""")
    ])
    
    synthesized = {}
    for category, data in all_data.items():
        # ìƒì„¸ ë°ì´í„° ìš”ì•½ (ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ)
        detailed_summary = []
        for item in data["detailed_data"][:3]:
            if item.get("results"):
                top_result = item["results"][0]
                detailed_summary.append(
                    f"- {top_result.get('title', '')}: {top_result.get('content', '')[:200]} (ì¶œì²˜: {top_result.get('url', '')})"
                )
        
        response = llm.invoke(
            prompt.format_messages(
                category=category,
                tavily_answers=data["tavily_answers"],
                detailed_data="\n".join(detailed_summary)
            )
        )
        synthesized[category] = response.content
    
    state["synthesized_data"] = synthesized
    state["messages"].append("âœ… ë°ì´í„° í†µí•© ë¶„ì„ ì™„ë£Œ (Tavily AI ë‹µë³€ í™œìš©)")
    return state

def quality_check_node(state: AgentState) -> str:
    """   ( )"""
    state.setdefault("search_context", {})
    categories = ["market", "tech", "industry", "company", "challenge"]
    diagnostics: List[str] = []
    failing: List[str] = []
    quality_feedback: Dict[str, List[str]] = {}

    for category in categories:
        data_key = f"{category}_data"
        entries = state.get(data_key, []) or []
        print(f"í’ˆì§ˆ ê²€ì‚¬ - {category}: {len(entries)} í•­ëª© ë¶„ì„ ì¤‘...")
        print(entries[:2])  # ì²˜ìŒ 2ê°œ í•­ëª© ì¶œë ¥
        total = len(entries)
        hits = sum(1 for item in entries if item.get("results"))
        answers = sum(1 for item in entries if item.get("answer"))

        failing_metrics: List[str] = []
        if total < QUALITY_CRITERIA["min_items"]:
            failing_metrics.append("items")
        if hits / total < QUALITY_CRITERIA["min_hit_ratio"]:
            failing_metrics.append("coverage")
        if answers / total < QUALITY_CRITERIA["min_answer_ratio"]:
            failing_metrics.append("answers")

        passes = total > 0 and (hits > 0 or answers > 0)

        if failing_metrics:
            quality_feedback[category] = failing_metrics

        diagnostics.append(
            f"{category}: total={total}, hits={hits}, answers={answers} -> {'PASS' if passes else 'RECHECK'}"
        )
        if not passes:
            failing.append(category)

    state["search_context"]["quality_feedback"] = quality_feedback
    state["search_context"]["quality_diagnostics"] = "\n".join(diagnostics)
    state["messages"].append("     - " + " | ".join(diagnostics))

    current_loop = state.get("iteration_count", 0) + 1
    
    state["iteration_count"] = current_loop 
    max_loops = 2
    
    if failing and current_loop < max_loops:
        print(f"í˜„ì¬ ë°˜ë³µ íšŸìˆ˜: {current_loop}")
        state["messages"].append(f"  : {', '.join(failing)} ( )")
        return "research_more"

    if failing:
        state["messages"].append(f"        : {', '.join(failing)} (  {max_loops}  )")

    state["messages"].append("    ")
    state["iteration_count"] = 0
    return "generate_report"

def report_generation_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„±"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    
    sections = {
        "executive_summary": "í•µì‹¬ ìš”ì•½",
        "market_overview": "ì‹œì¥ ì „ë§",
        "technology_trends": "ê¸°ìˆ  íŠ¸ë Œë“œ",
        "industry_applications": "ì‚°ì—…ë³„ ì‘ìš©",
        "key_players": "ì£¼ìš” ê¸°ì—…",
        "challenges": "ë„ì „ê³¼ì œ",
        "forecast": "í–¥í›„ 5ë…„ ì „ë§",
        "recommendations": "ì „ëµì  ê¶Œê³ ì‚¬í•­"
    }
    
    report_sections = {}
    
    for section_key, section_title in sections.items():
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""Physical AI íŠ¸ë Œë“œ ë³´ê³ ì„œì˜ '{section_title}' ì„¹ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”.
            ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ë©° ë°ì´í„° ê¸°ë°˜ì˜ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""),
            ("user", "ë¶„ì„ ë°ì´í„°:\n{data}")
        ])
        
        response = llm.invoke(
            prompt.format_messages(data=str(state["synthesized_data"]))
        )
        report_sections[section_key] = response.content
    
    state["report_sections"] = report_sections
    state["messages"].append("âœ… ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ")
    return state

def structure_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ êµ¬ì¡°í™” ë° í¬ë§·íŒ…"""
    sections = state["report_sections"]
    
    # Markdown í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ êµ¬ì¡°í™”
    final_report = f"""# Physical AI íŠ¸ë Œë“œ ì˜ˆì¸¡ ë³´ê³ ì„œ (2025-2030)

# í•µì‹¬ ìš”ì•½
{sections.get('executive_summary', '')}

---

# 1. ì‹œì¥ ì „ë§
{sections.get('market_overview', '')}

# 2. í•µì‹¬ ê¸°ìˆ  íŠ¸ë Œë“œ
{sections.get('technology_trends', '')}

# 3. ì‚°ì—…ë³„ ì‘ìš© ë¶„ì•¼
{sections.get('industry_applications', '')}

# 4. ì£¼ìš” ê¸°ì—… ë° ê²½ìŸ í™˜ê²½
{sections.get('key_players', '')}

# 5. ë„ì „ê³¼ì œ ë° ì¥ë²½
{sections.get('challenges', '')}

# 6. í–¥í›„ 5ë…„ ì „ë§
{sections.get('forecast', '')}

# 7. ê¸°ì—…ì„ ìœ„í•œ ì „ëµì  ê¶Œê³ ì‚¬í•­
{sections.get('recommendations', '')}

---
*ë³´ê³ ì„œ ìƒì„±ì¼: {__import__('datetime').datetime.now().strftime('%Y-%m-%d')}*
"""
    
    state["final_report"] = final_report
    state["messages"].append("âœ… ë³´ê³ ì„œ êµ¬ì¡°í™” ì™„ë£Œ")
    return state

def review_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ í’ˆì§ˆ ê²€í† """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    response = llm.invoke(
        REVIEW_FEW_SHOT_PROMPT.format_messages(
            baseline_report=REVIEW_BASELINE_REPORT,
            strong_report=REVIEW_STRONG_REPORT,
            report=state["final_report"][:4000]
        )
    )

    content = response.content.strip()
    score_match = re.search(r"ì ìˆ˜\s*[:=]\s*(\d+(?:\.\d+)?)", content)
    score = float(score_match.group(1)) if score_match else 7.0
    score = max(0.0, min(10.0, score))

    state.setdefault("search_context", {})
    state["search_context"]["review_feedback"] = content
    print(f"ë³´ê³ ì„œ í’ˆì§ˆ ì ìˆ˜: {score}/10")
    print("ë³´ê³ ì„œ í’ˆì§ˆ ê²€í†  ë‚´ìš©:", content)
    state["quality_score"] = score
    state["messages"].append(f"âœ… í’ˆì§ˆ ê²€í†  ì™„ë£Œ (ì ìˆ˜: {score:.1f}/10)")
    state["messages"].append("ğŸ“ ë¦¬ë·° ìš”ì•½ ì €ì¥")
    return state

def final_quality_check_node(state: AgentState) -> str:
    """ìµœì¢… í’ˆì§ˆ í™•ì¸"""
    if state["quality_score"] < 7.0 and state["iteration_count"] < 2:
        state["iteration_count"] += 1
        return "refine"
    return "format"

def refinement_node(state: AgentState) -> AgentState:
    """ë³´ê³ ì„œ ê°œì„ """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë³´ê³ ì„œë¥¼ ê°œì„ í•˜ì„¸ìš”. ë‹¤ìŒì— ì§‘ì¤‘í•˜ì„¸ìš”:
        1. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë° ë°ì´í„° ì¶”ê°€
        2. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­ ê°•í™”
        3. ë…¼ë¦¬ì  íë¦„ ê°œì„ """),
        ("user", "í˜„ì¬ ë³´ê³ ì„œ:\n{report}\n\në¶„ì„ ë°ì´í„°:\n{data}")
    ])
    
    response = llm.invoke(
        prompt.format_messages(
            report=state["final_report"][:2000],
            data=str(state["synthesized_data"])[:1000]
        )
    )
    
    # ê°œì„ ëœ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    state["final_report"] = response.content
    state["messages"].append("âœ… ë³´ê³ ì„œ ê°œì„  ì™„ë£Œ")
    return state

def formatting_node(state: AgentState) -> AgentState:
    """ìµœì¢… í¬ë§·íŒ… ë° PDF ìƒì„±"""
    state["messages"].append("âœ… ìµœì¢… í¬ë§·íŒ… ì™„ë£Œ")
    
    # PDF ìƒì„±
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_filename = f"physical_ai_report_{timestamp}.pdf"
        markdown_to_pdf(state["final_report"], pdf_filename)
        state["messages"].append(f"ğŸ“„ PDF ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {pdf_filename}")
    except Exception as e:
        state["messages"].append(f"âš ï¸ PDF ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    return state

# ==================== ê·¸ë˜í”„ êµ¬ì„± ====================

def create_physical_ai_agent():
    """Physical AI ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„±"""
    
    # StateGraph ì´ˆê¸°í™”
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("planning", planning_node)
    workflow.add_node("market_research", market_research_node)
    workflow.add_node("tech_research", tech_research_node)
    workflow.add_node("industry_research", industry_research_node)
    workflow.add_node("company_research", company_research_node)
    workflow.add_node("challenge_research", challenge_research_node)
    workflow.add_node("synthesis", synthesis_node)
    workflow.add_node("report_generation", report_generation_node)
    workflow.add_node("structure", structure_node)
    workflow.add_node("review", review_node)
    workflow.add_node("refinement", refinement_node)
    workflow.add_node("formatting", formatting_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("planning")
    
    # Planning -> Research ë³‘ë ¬ ì‹¤í–‰
    workflow.add_edge("planning", "market_research")
    workflow.add_edge("planning", "tech_research")
    workflow.add_edge("planning", "industry_research")
    workflow.add_edge("planning", "company_research")
    workflow.add_edge("planning", "challenge_research")
    
    # Research -> Synthesis
    workflow.add_edge("market_research", "synthesis")
    workflow.add_edge("tech_research", "synthesis")
    workflow.add_edge("industry_research", "synthesis")
    workflow.add_edge("company_research", "synthesis")
    workflow.add_edge("challenge_research", "synthesis")
    
    # Synthesis -> Quality Check (ì¡°ê±´ë¶€)
    workflow.add_conditional_edges(
        "synthesis",
        quality_check_node,
        {
            "research_more": "planning",
            "generate_report": "report_generation"
        }
    )
    
    # Report Generation -> Structure
    workflow.add_edge("report_generation", "structure")
    
    # Structure -> Review
    workflow.add_edge("structure", "review")
    
    # Review -> Final Check (ì¡°ê±´ë¶€)
    workflow.add_conditional_edges(
        "review",
        final_quality_check_node,
        {
            "refine": "refinement",
            "format": "formatting"
        }
    )
    
    # Refinement -> Structure (ê°œì„  í›„ ì¬ê²€í† )
    workflow.add_edge("refinement", "structure")
    
    # Formatting -> END
    workflow.add_edge("formatting", END)
    
    return workflow.compile()

# ==================== ì‹¤í–‰ ì˜ˆì œ ====================

def run_agent(user_query: str):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    
    # Tavily API í‚¤ í™•ì¸
    if not os.environ.get("TAVILY_API_KEY"):
        print("âš ï¸  ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("https://tavily.com ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
        return None
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "user_query": user_query,
        "research_plan": {},
        "market_data": [],
        "tech_data": [],
        "industry_data": [],
        "company_data": [],
        "challenge_data": [],
        "synthesized_data": {},
        "report_sections": {},
        "final_report": "",
        "quality_score": 0.0,
        "iteration_count": 0,
        "search_context": {},  # Tavily AI ë‹µë³€ ì €ì¥
        "messages": []
    }
    
    print("ğŸš€ Physical AI ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
    print(f"ğŸ“ ìš”ì²­: {user_query}\n")
    
    # ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰
    agent = create_physical_ai_agent()
    result = agent.invoke(initial_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ì‹¤í–‰ ë¡œê·¸")
    print("="*60)
    for msg in result["messages"]:
        print(msg)
    
    print("\n" + "="*60)
    print("ğŸ“„ ìµœì¢… ë³´ê³ ì„œ")
    print("="*60)
    print(result["final_report"])
    
    print("\n" + "="*60)
    print("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ˆ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/10")
    print(f"ğŸ”„ ë°˜ë³µ íšŸìˆ˜: {result['iteration_count']}")
    print("="*60)
    
    return result

# ì‹¤í–‰
if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”
    # export OPENAI_API_KEY="your-openai-key"
    # export TAVILY_API_KEY="your-tavily-key"
    
    query = "í–¥í›„ 5ë…„ ì´ë‚´ ê¸°ì—…ì—ì„œ ê´€ì‹¬ìˆê²Œ ë´ì•¼í•  Physical AI íŠ¸ë Œë“œ"
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Physical AI íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸              â•‘
â•‘              Powered by LangGraph + Tavily                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    result = run_agent(query)
    
    if result:
        # ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"physical_ai_report_{timestamp}.md"
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(result["final_report"])
        
        print(f"\nğŸ’¾ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")